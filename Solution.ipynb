{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving up text... Shakespeare style!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief: Using LSTM models to predict text based on Shakespeare sonnets. This will be based character based training rather than words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('shakespeare.txt','r')\n",
    "all_text=f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_corpus=set(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocab is 84\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the vocab is {len(set_corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(all_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using characters instead of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating integer values for each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_decoder(text):\n",
    "    '''\n",
    "    # Dict {index:letter} for every letter in text\n",
    "    '''\n",
    "    decoded_dict=(dict(enumerate(set(text))))\n",
    "    return decoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=dict_decoder(all_text) # Dict {index:letter} #Creating the dictionary for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_encoder(text):\n",
    "    '''\n",
    "    # Dict {letter:index} for every letter in text\n",
    "    '''\n",
    "    dec_text= dict_decoder(text)\n",
    "    encoded_dict = dict([(word,i) for i,word in dec_text.items()])\n",
    "    return encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_text(text,encoder):\n",
    "    return [encoder[word] for word in text]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= dict_encoder(all_text) #Creating the dictionary for encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_text=encoding_text(all_text, encoder) #Representing the corpus with encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(encode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Creating individual steps before consolidating into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 40\n",
    "samples_per_batch=30\n",
    "chars_per_batch=window*samples_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4538"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_count=int(len(encode_text)/chars_per_batch)\n",
    "batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5445609\n",
      "5445600\n"
     ]
    }
   ],
   "source": [
    "text_corp=encode_text[:batch_count*chars_per_batch]\n",
    "print(len(encode_text))\n",
    "print(len(text_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445600,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(text_corp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 181520)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(181520,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corp=np.array(text_corp).reshape((samples_per_batch,-1))\n",
    "print(text_corp.shape)\n",
    "text_corp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whoopsie! Crossed index limits\n"
     ]
    }
   ],
   "source": [
    "batch=[]\n",
    "i=0\n",
    "for samp in range(0,text_corp.shape[1],window):\n",
    "    x=text_corp[:,samp:samp+window]\n",
    "    y = np.zeros_like(x)\n",
    "    try:\n",
    "        y[:,:-1]=x[:,1:]\n",
    "        y[:,-1]=text_corp[:,samp+window][0]\n",
    "    except:\n",
    "        print('Whoopsie! Crossed index limits')\n",
    "        y[:,:-1]=x[:,1:]\n",
    "        y[:,-1]=text_corp[:,0][0]\n",
    "    batch.append((x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Consolidating all of the above into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(samp_encoded_text,samp_batch=30,window=40, encoder=encoder): #We have to use the encoded text directly since the train and test batches are based on encoded text\n",
    "    '''\n",
    "    Args:\n",
    "    all_text: corpus of text\n",
    "    samp_batch: how many samples of text should one batch have\n",
    "    window: window, length of one sample (how many characters should be passed to the LSTM i.e. how many cells in the LSTM)\n",
    "    Given a set of text:\n",
    "    1. Encode the text based on the dictionary provided\n",
    "    2. Based on the number of samples per batch and the length of each sequence (the window) of the sample, calculate the number of characters we get per batch i.e. chars_per_batch=window*samples_per_batch\n",
    "    3. Since, we know what the total number of characters in the text is, we can find out how many batches we will get (rounded down to the lower integer value) as: batch_count = Quotient(total number of characters / characters per batch).\n",
    "    4. We then take only that many characters that is an integer multiple of batch i.e. final corpus = initial corpus[batch_count*characters per batch]. Hence, we now get a list which is batch_count*window*samples_per_batch\n",
    "    5. Now we can reshape this into a matrix of [samples_per_batch, window*batch_count]. Hence, this is a matrix of batch_count number of data sets each of window long stacked horizontally). We can then slice this column-wise to get all our batches of [samples_per_batch, window]\n",
    "    returns:\n",
    "    x: set of inputs\n",
    "    y:set of outputs\n",
    "    '''\n",
    "    encode_samp=samp_encoded_text # Step 1 described above\n",
    "#     print('len of encode_samp',len(encode_samp))\n",
    "#     print('len of encode_text',len(sample_text))\n",
    "    window = window #Extracting window size \n",
    "    samples_per_batch=samp_batch #Extracting samples per batch\n",
    "    chars_per_batch=window*samples_per_batch #Step 2 described above\n",
    "    batch_count=int(len(encode_samp)/chars_per_batch) #Step 3 described above\n",
    "    final_corpus=encode_samp[:batch_count*chars_per_batch] #Step 4 described above\n",
    "#     print('final corpus',final_corpus)\n",
    "#     print('final corpus length',len(final_corpus))\n",
    "    final_corpus = np.array(final_corpus).reshape((samples_per_batch,-1))\n",
    "    \n",
    "    # Slicing final_corpus to provide the batches:\n",
    "    for samp in range(0,final_corpus.shape[1],window):\n",
    "        x=final_corpus[:,samp:samp+window]\n",
    "        y=np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1]=x[:,1:]\n",
    "            y[:,-1]=final_corpus[:,samp+window]\n",
    "        except:\n",
    "            y[:,:-1]=x[:,1:]\n",
    "            y[:,-1]=final_corpus[:,0]\n",
    "        yield (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Testing on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = all_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_encode=encoding_text(sample_text, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_samp=create_batches(samp_encode,samp_batch=2,window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(batches_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59, 40, 40, 40, 40],\n",
       "       [40, 40, 40, 40, 40]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 40, 40, 40, 40],\n",
       "       [40, 40, 40, 40, 40]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,text,num_hidden=256,num_layers=4,batch_first=True,dropout_prob=0.5,use_gpu=True):\n",
    "        #currently the input size is [number of samples / batch, seq_len]. However, as per LSTM model, the inputs need to be given as tensor of shape (seq_len, samples/batch, input size) when batch_first=False or (samples/batch, seq_len, input_size) when batch_first=True. Hence, we can set batch_first=True and use x directly for first two dimensions. The third dimension (input size) will be generated automatically when we convert X to a one-hot coded vector)\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers=num_layers\n",
    "        self.batch_first=batch_first\n",
    "        self.dropout_p=dropout_prob\n",
    "        self.text=text\n",
    "        self.use_gpu=use_gpu\n",
    "        self.lstm=nn.LSTM(input_size=len(self.text),\n",
    "                             hidden_size=self.num_hidden,\n",
    "                             num_layers=self.num_layers,\n",
    "                             batch_first=self.batch_first, \n",
    "                             dropout=self.dropout_p)\n",
    "        self.dropout=nn.Dropout(self.dropout_p)\n",
    "        self.fc1=nn.Linear(num_hidden,len(self.text))\n",
    "        \n",
    "    def forward(self,X,hidden_input):\n",
    "        output, hidden_input = self.lstm(X,hidden_input)      \n",
    "        drop_out=self.dropout(output)\n",
    "        drop_out=drop_out.reshape(-1,self.num_hidden)\n",
    "        out=self.fc1(drop_out)\n",
    "        return out, hidden_input\n",
    "    \n",
    "    def create_hidden(self,batch_size):\n",
    "        '''As per Pytorch documentation, the hidden initial state and initial cell state need to be passed to the forward function as a tuple.\n",
    "        Initial hidden state needs to be given as [num_layers,no of samples/batch, Hout - number of outputs to next hidden layer which we have all set as the same number of nodes.]\n",
    "        Initial cell state needs to be given as [num_layers,no of samples/batch, Hout - number of outputs to next hidden layer which we have all set as the same number of nodes.]\n",
    "        '''\n",
    "        if self.use_gpu:\n",
    "            hidden_input=(torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                    torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden_input=(torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                    torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        return hidden_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(text=set_corpus,\n",
    "                       num_hidden=512,\n",
    "                       num_layers=5,\n",
    "                       batch_first=True,\n",
    "                       dropout_prob=0.3,\n",
    "                       use_gpu=torch.cuda.is_available()\n",
    "                      )\n",
    "if torch.cuda.is_available():\n",
    "    lstm_model.cuda()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(lstm_model.parameters(),lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of LSTMModel(\n",
      "  (lstm): LSTM(84, 512, num_layers=5, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=512, out_features=84, bias=True)\n",
      ")>\n",
      "0:\t 172032\n",
      "1:\t 1048576\n",
      "2:\t 2048\n",
      "3:\t 2048\n",
      "4:\t 1048576\n",
      "5:\t 1048576\n",
      "6:\t 2048\n",
      "7:\t 2048\n",
      "8:\t 1048576\n",
      "9:\t 1048576\n",
      "10:\t 2048\n",
      "11:\t 2048\n",
      "12:\t 1048576\n",
      "13:\t 1048576\n",
      "14:\t 2048\n",
      "15:\t 2048\n",
      "16:\t 1048576\n",
      "17:\t 1048576\n",
      "18:\t 2048\n",
      "19:\t 2048\n",
      "20:\t 43008\n",
      "21:\t 84\n",
      "Total number of parameters: 9672788\n"
     ]
    }
   ],
   "source": [
    "print(lstm_model.parameters)\n",
    "parame=[]\n",
    "for e,parameter in enumerate(lstm_model.parameters()):\n",
    "    print(f'{e}:\\t {parameter.numel()}')\n",
    "    parame.append(parameter.numel())\n",
    "print(f'Total number of parameters: {np.array(parame).sum()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding for X as per LSTM input requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(array,vocab_size):\n",
    "    one_hot=torch.zeros(array.shape[0],array.shape[1],vocab_size)\n",
    "    for a in range(array.shape[0]):\n",
    "        for b in range(array.shape[1]):\n",
    "            one_hot[a,b,array[a,b]]=1 \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr=np.array([[1,2,0],[2,5,6]])\n",
    "print(arr.shape)\n",
    "one_hot_encoding(arr,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider using:\n",
    "batch_size = 64\n",
    "seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No. of batches available 850\n"
     ]
    }
   ],
   "source": [
    "number_batches=len(all_text)/(batch_size*seq_len)\n",
    "print(f' No. of batches available {int(number_batches)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <i>If we have only 425 sets of data to train on, we need to maximize the train data size and minimize the test data size adequately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text length: 5445609\n",
      "Encoded train length: 4901048\n",
      "Encoded test length: 544561\n"
     ]
    }
   ],
   "source": [
    "train_len=int(len(encode_text)*(1-split))\n",
    "test_len=len(encode_text)-train_len\n",
    "print(f'Encoded text length: {len(encode_text)}')\n",
    "print(f'Encoded train length: {train_len}')\n",
    "print(f'Encoded test length: {test_len}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=encode_text[:train_len]\n",
    "test_data=encode_text[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing how to extract hidden tuple values\n",
    "test_hidden_input=(torch.zeros(4,2,5), #self.num_layers,batch_size,self.num_hidden\n",
    "                    torch.zeros(4,2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_hidden_input=tuple([i for i in test_hidden_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hidden_input==test2_hidden_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we on cuda? True\n",
      "TRAIN: batch:50,epoch:0,loss:3.189709424972534\n",
      "TRAIN: batch:100,epoch:0,loss:3.2035446166992188\n",
      "TRAIN: batch:150,epoch:0,loss:3.2277395725250244\n",
      "TRAIN: batch:200,epoch:0,loss:3.1316146850585938\n",
      "TRAIN: batch:250,epoch:0,loss:2.921280860900879\n",
      "TRAIN: batch:300,epoch:0,loss:2.7137057781219482\n",
      "TRAIN: batch:350,epoch:0,loss:2.556753635406494\n",
      "TRAIN: batch:400,epoch:0,loss:2.473602056503296\n",
      "TRAIN: batch:450,epoch:0,loss:2.3357155323028564\n",
      "TRAIN: batch:500,epoch:0,loss:2.3206005096435547\n",
      "TRAIN: batch:550,epoch:0,loss:2.294593572616577\n",
      "TRAIN: batch:600,epoch:0,loss:2.2325544357299805\n",
      "TRAIN: batch:650,epoch:0,loss:2.2014055252075195\n",
      "TRAIN: batch:700,epoch:0,loss:2.1719839572906494\n",
      "TRAIN: batch:750,epoch:0,loss:2.1745834350585938\n",
      "TEST: batch:10,epoch:0,loss:2.144775867462158\n",
      "TEST: batch:20,epoch:0,loss:2.1482796669006348\n",
      "TEST: batch:30,epoch:0,loss:2.136326551437378\n",
      "TEST: batch:40,epoch:0,loss:2.153010368347168\n",
      "TEST: batch:50,epoch:0,loss:2.1546497344970703\n",
      "TEST: batch:60,epoch:0,loss:2.1381027698516846\n",
      "TEST: batch:70,epoch:0,loss:2.0986742973327637\n",
      "TEST: batch:80,epoch:0,loss:2.1841988563537598\n",
      "TRAIN: batch:50,epoch:1,loss:2.052452564239502\n",
      "TRAIN: batch:100,epoch:1,loss:2.0706708431243896\n",
      "TRAIN: batch:150,epoch:1,loss:2.061876058578491\n",
      "TRAIN: batch:200,epoch:1,loss:2.024171829223633\n",
      "TRAIN: batch:250,epoch:1,loss:1.9996609687805176\n",
      "TRAIN: batch:300,epoch:1,loss:1.9828790426254272\n",
      "TRAIN: batch:350,epoch:1,loss:1.9071508646011353\n",
      "TRAIN: batch:400,epoch:1,loss:1.9642910957336426\n",
      "TRAIN: batch:450,epoch:1,loss:1.902442455291748\n",
      "TRAIN: batch:500,epoch:1,loss:1.845556378364563\n",
      "TRAIN: batch:550,epoch:1,loss:1.8319330215454102\n",
      "TRAIN: batch:600,epoch:1,loss:1.8606473207473755\n",
      "TRAIN: batch:650,epoch:1,loss:1.8121511936187744\n",
      "TRAIN: batch:700,epoch:1,loss:1.8026307821273804\n",
      "TRAIN: batch:750,epoch:1,loss:1.7664753198623657\n",
      "TEST: batch:10,epoch:1,loss:1.8253543376922607\n",
      "TEST: batch:20,epoch:1,loss:1.8383498191833496\n",
      "TEST: batch:30,epoch:1,loss:1.8179562091827393\n",
      "TEST: batch:40,epoch:1,loss:1.7844672203063965\n",
      "TEST: batch:50,epoch:1,loss:1.7993513345718384\n",
      "TEST: batch:60,epoch:1,loss:1.7872593402862549\n",
      "TEST: batch:70,epoch:1,loss:1.7153980731964111\n",
      "TEST: batch:80,epoch:1,loss:1.8664939403533936\n",
      "TRAIN: batch:50,epoch:2,loss:1.6542091369628906\n",
      "TRAIN: batch:100,epoch:2,loss:1.7296732664108276\n",
      "TRAIN: batch:150,epoch:2,loss:1.7312977313995361\n",
      "TRAIN: batch:200,epoch:2,loss:1.727736234664917\n",
      "TRAIN: batch:250,epoch:2,loss:1.7032278776168823\n",
      "TRAIN: batch:300,epoch:2,loss:1.6919649839401245\n",
      "TRAIN: batch:350,epoch:2,loss:1.6444175243377686\n",
      "TRAIN: batch:400,epoch:2,loss:1.701766848564148\n",
      "TRAIN: batch:450,epoch:2,loss:1.669713020324707\n",
      "TRAIN: batch:500,epoch:2,loss:1.618263602256775\n",
      "TRAIN: batch:550,epoch:2,loss:1.563990831375122\n",
      "TRAIN: batch:600,epoch:2,loss:1.6408202648162842\n",
      "TRAIN: batch:650,epoch:2,loss:1.6089519262313843\n",
      "TRAIN: batch:700,epoch:2,loss:1.6076356172561646\n",
      "TRAIN: batch:750,epoch:2,loss:1.5627641677856445\n",
      "TEST: batch:10,epoch:2,loss:1.671147108078003\n",
      "TEST: batch:20,epoch:2,loss:1.683599829673767\n",
      "TEST: batch:30,epoch:2,loss:1.6630332469940186\n",
      "TEST: batch:40,epoch:2,loss:1.6214027404785156\n",
      "TEST: batch:50,epoch:2,loss:1.6271791458129883\n",
      "TEST: batch:60,epoch:2,loss:1.6202609539031982\n",
      "TEST: batch:70,epoch:2,loss:1.536458134651184\n",
      "TEST: batch:80,epoch:2,loss:1.7249301671981812\n",
      "TRAIN: batch:50,epoch:3,loss:1.4758398532867432\n",
      "TRAIN: batch:100,epoch:3,loss:1.5642924308776855\n",
      "TRAIN: batch:150,epoch:3,loss:1.5551207065582275\n",
      "TRAIN: batch:200,epoch:3,loss:1.560917854309082\n",
      "TRAIN: batch:250,epoch:3,loss:1.5442705154418945\n",
      "TRAIN: batch:300,epoch:3,loss:1.5518299341201782\n",
      "TRAIN: batch:350,epoch:3,loss:1.4882407188415527\n",
      "TRAIN: batch:400,epoch:3,loss:1.5637197494506836\n",
      "TRAIN: batch:450,epoch:3,loss:1.5389344692230225\n",
      "TRAIN: batch:500,epoch:3,loss:1.4972906112670898\n",
      "TRAIN: batch:550,epoch:3,loss:1.4195863008499146\n",
      "TRAIN: batch:600,epoch:3,loss:1.5259153842926025\n",
      "TRAIN: batch:650,epoch:3,loss:1.4860508441925049\n",
      "TRAIN: batch:700,epoch:3,loss:1.491918683052063\n",
      "TRAIN: batch:750,epoch:3,loss:1.4383878707885742\n",
      "TEST: batch:10,epoch:3,loss:1.5852351188659668\n",
      "TEST: batch:20,epoch:3,loss:1.5916306972503662\n",
      "TEST: batch:30,epoch:3,loss:1.5542080402374268\n",
      "TEST: batch:40,epoch:3,loss:1.5389460325241089\n",
      "TEST: batch:50,epoch:3,loss:1.5415252447128296\n",
      "TEST: batch:60,epoch:3,loss:1.5215601921081543\n",
      "TEST: batch:70,epoch:3,loss:1.4390356540679932\n",
      "TEST: batch:80,epoch:3,loss:1.6211285591125488\n",
      "TRAIN: batch:50,epoch:4,loss:1.3749712705612183\n",
      "TRAIN: batch:100,epoch:4,loss:1.4587138891220093\n",
      "TRAIN: batch:150,epoch:4,loss:1.447791337966919\n",
      "TRAIN: batch:200,epoch:4,loss:1.4740699529647827\n",
      "TRAIN: batch:250,epoch:4,loss:1.4526809453964233\n",
      "TRAIN: batch:300,epoch:4,loss:1.449312686920166\n",
      "TRAIN: batch:350,epoch:4,loss:1.411238431930542\n",
      "TRAIN: batch:400,epoch:4,loss:1.4576166868209839\n",
      "TRAIN: batch:450,epoch:4,loss:1.4602627754211426\n",
      "TRAIN: batch:500,epoch:4,loss:1.4211732149124146\n",
      "TRAIN: batch:550,epoch:4,loss:1.335820198059082\n",
      "TRAIN: batch:600,epoch:4,loss:1.432857871055603\n",
      "TRAIN: batch:650,epoch:4,loss:1.402587890625\n",
      "TRAIN: batch:700,epoch:4,loss:1.4119174480438232\n",
      "TRAIN: batch:750,epoch:4,loss:1.3712173700332642\n",
      "TEST: batch:10,epoch:4,loss:1.525610327720642\n",
      "TEST: batch:20,epoch:4,loss:1.535405158996582\n",
      "TEST: batch:30,epoch:4,loss:1.5040684938430786\n",
      "TEST: batch:40,epoch:4,loss:1.4904650449752808\n",
      "TEST: batch:50,epoch:4,loss:1.4761879444122314\n",
      "TEST: batch:60,epoch:4,loss:1.468063473701477\n",
      "TEST: batch:70,epoch:4,loss:1.38954758644104\n",
      "TEST: batch:80,epoch:4,loss:1.565771222114563\n",
      "TRAIN: batch:50,epoch:5,loss:1.3200922012329102\n",
      "TRAIN: batch:100,epoch:5,loss:1.394985556602478\n",
      "TRAIN: batch:150,epoch:5,loss:1.3688746690750122\n",
      "TRAIN: batch:200,epoch:5,loss:1.4023263454437256\n",
      "TRAIN: batch:250,epoch:5,loss:1.3886011838912964\n",
      "TRAIN: batch:300,epoch:5,loss:1.398516058921814\n",
      "TRAIN: batch:350,epoch:5,loss:1.350427508354187\n",
      "TRAIN: batch:400,epoch:5,loss:1.408592700958252\n",
      "TRAIN: batch:450,epoch:5,loss:1.4093983173370361\n",
      "TRAIN: batch:500,epoch:5,loss:1.364859938621521\n",
      "TRAIN: batch:550,epoch:5,loss:1.2804008722305298\n",
      "TRAIN: batch:600,epoch:5,loss:1.3891454935073853\n",
      "TRAIN: batch:650,epoch:5,loss:1.3615933656692505\n",
      "TRAIN: batch:700,epoch:5,loss:1.3632277250289917\n",
      "TRAIN: batch:750,epoch:5,loss:1.3202930688858032\n",
      "TEST: batch:10,epoch:5,loss:1.4836326837539673\n",
      "TEST: batch:20,epoch:5,loss:1.5005102157592773\n",
      "TEST: batch:30,epoch:5,loss:1.4667410850524902\n",
      "TEST: batch:40,epoch:5,loss:1.449355125427246\n",
      "TEST: batch:50,epoch:5,loss:1.4456554651260376\n",
      "TEST: batch:60,epoch:5,loss:1.4277218580245972\n",
      "TEST: batch:70,epoch:5,loss:1.3407104015350342\n",
      "TEST: batch:80,epoch:5,loss:1.5248210430145264\n",
      "TRAIN: batch:50,epoch:6,loss:1.2768428325653076\n",
      "TRAIN: batch:100,epoch:6,loss:1.3396148681640625\n",
      "TRAIN: batch:150,epoch:6,loss:1.324917197227478\n",
      "TRAIN: batch:200,epoch:6,loss:1.357144832611084\n",
      "TRAIN: batch:250,epoch:6,loss:1.347975254058838\n",
      "TRAIN: batch:300,epoch:6,loss:1.3537005186080933\n",
      "TRAIN: batch:350,epoch:6,loss:1.2905786037445068\n",
      "TRAIN: batch:400,epoch:6,loss:1.3661963939666748\n",
      "TRAIN: batch:450,epoch:6,loss:1.3690345287322998\n",
      "TRAIN: batch:500,epoch:6,loss:1.323705792427063\n",
      "TRAIN: batch:550,epoch:6,loss:1.235395073890686\n",
      "TRAIN: batch:600,epoch:6,loss:1.3472172021865845\n",
      "TRAIN: batch:650,epoch:6,loss:1.324642539024353\n",
      "TRAIN: batch:700,epoch:6,loss:1.3322021961212158\n",
      "TRAIN: batch:750,epoch:6,loss:1.270919919013977\n",
      "TEST: batch:10,epoch:6,loss:1.4518775939941406\n",
      "TEST: batch:20,epoch:6,loss:1.472610592842102\n",
      "TEST: batch:30,epoch:6,loss:1.4396958351135254\n",
      "TEST: batch:40,epoch:6,loss:1.423569917678833\n",
      "TEST: batch:50,epoch:6,loss:1.4201931953430176\n",
      "TEST: batch:60,epoch:6,loss:1.398300051689148\n",
      "TEST: batch:70,epoch:6,loss:1.3141005039215088\n",
      "TEST: batch:80,epoch:6,loss:1.4944789409637451\n",
      "TRAIN: batch:50,epoch:7,loss:1.2491754293441772\n",
      "TRAIN: batch:100,epoch:7,loss:1.3071446418762207\n",
      "TRAIN: batch:150,epoch:7,loss:1.2919440269470215\n",
      "TRAIN: batch:200,epoch:7,loss:1.3313888311386108\n",
      "TRAIN: batch:250,epoch:7,loss:1.3178192377090454\n",
      "TRAIN: batch:300,epoch:7,loss:1.3067625761032104\n",
      "TRAIN: batch:350,epoch:7,loss:1.2707945108413696\n",
      "TRAIN: batch:400,epoch:7,loss:1.3232321739196777\n",
      "TRAIN: batch:450,epoch:7,loss:1.3421610593795776\n",
      "TRAIN: batch:500,epoch:7,loss:1.2953002452850342\n",
      "TRAIN: batch:550,epoch:7,loss:1.2125760316848755\n",
      "TRAIN: batch:600,epoch:7,loss:1.317948579788208\n",
      "TRAIN: batch:650,epoch:7,loss:1.2981945276260376\n",
      "TRAIN: batch:700,epoch:7,loss:1.2993677854537964\n",
      "TRAIN: batch:750,epoch:7,loss:1.2477449178695679\n",
      "TEST: batch:10,epoch:7,loss:1.4350031614303589\n",
      "TEST: batch:20,epoch:7,loss:1.4580748081207275\n",
      "TEST: batch:30,epoch:7,loss:1.4170432090759277\n",
      "TEST: batch:40,epoch:7,loss:1.4071704149246216\n",
      "TEST: batch:50,epoch:7,loss:1.4036637544631958\n",
      "TEST: batch:60,epoch:7,loss:1.3798911571502686\n",
      "TEST: batch:70,epoch:7,loss:1.3025217056274414\n",
      "TEST: batch:80,epoch:7,loss:1.47310209274292\n",
      "TRAIN: batch:50,epoch:8,loss:1.224029541015625\n",
      "TRAIN: batch:100,epoch:8,loss:1.2704907655715942\n",
      "TRAIN: batch:150,epoch:8,loss:1.270660161972046\n",
      "TRAIN: batch:200,epoch:8,loss:1.3001173734664917\n",
      "TRAIN: batch:250,epoch:8,loss:1.2809436321258545\n",
      "TRAIN: batch:300,epoch:8,loss:1.2929787635803223\n",
      "TRAIN: batch:350,epoch:8,loss:1.2536931037902832\n",
      "TRAIN: batch:400,epoch:8,loss:1.2960647344589233\n",
      "TRAIN: batch:450,epoch:8,loss:1.320278286933899\n",
      "TRAIN: batch:500,epoch:8,loss:1.276615858078003\n",
      "TRAIN: batch:550,epoch:8,loss:1.184612512588501\n",
      "TRAIN: batch:600,epoch:8,loss:1.2930283546447754\n",
      "TRAIN: batch:650,epoch:8,loss:1.2788125276565552\n",
      "TRAIN: batch:700,epoch:8,loss:1.26699697971344\n",
      "TRAIN: batch:750,epoch:8,loss:1.2188377380371094\n",
      "TEST: batch:10,epoch:8,loss:1.4243626594543457\n",
      "TEST: batch:20,epoch:8,loss:1.4364874362945557\n",
      "TEST: batch:30,epoch:8,loss:1.3948335647583008\n",
      "TEST: batch:40,epoch:8,loss:1.3991591930389404\n",
      "TEST: batch:50,epoch:8,loss:1.3855170011520386\n",
      "TEST: batch:60,epoch:8,loss:1.3626165390014648\n",
      "TEST: batch:70,epoch:8,loss:1.2780661582946777\n",
      "TEST: batch:80,epoch:8,loss:1.472725510597229\n",
      "TRAIN: batch:50,epoch:9,loss:1.2009943723678589\n",
      "TRAIN: batch:100,epoch:9,loss:1.2513790130615234\n",
      "TRAIN: batch:150,epoch:9,loss:1.2474769353866577\n",
      "TRAIN: batch:200,epoch:9,loss:1.278741717338562\n",
      "TRAIN: batch:250,epoch:9,loss:1.2571418285369873\n",
      "TRAIN: batch:300,epoch:9,loss:1.2636191844940186\n",
      "TRAIN: batch:350,epoch:9,loss:1.2175768613815308\n",
      "TRAIN: batch:400,epoch:9,loss:1.2796587944030762\n",
      "TRAIN: batch:450,epoch:9,loss:1.2983043193817139\n",
      "TRAIN: batch:500,epoch:9,loss:1.2566163539886475\n",
      "TRAIN: batch:550,epoch:9,loss:1.1664965152740479\n",
      "TRAIN: batch:600,epoch:9,loss:1.2718920707702637\n",
      "TRAIN: batch:650,epoch:9,loss:1.2486236095428467\n",
      "TRAIN: batch:700,epoch:9,loss:1.2658483982086182\n",
      "TRAIN: batch:750,epoch:9,loss:1.2054928541183472\n",
      "TEST: batch:10,epoch:9,loss:1.4152010679244995\n",
      "TEST: batch:20,epoch:9,loss:1.4227590560913086\n",
      "TEST: batch:30,epoch:9,loss:1.3893380165100098\n",
      "TEST: batch:40,epoch:9,loss:1.3902336359024048\n",
      "TEST: batch:50,epoch:9,loss:1.3602734804153442\n",
      "TEST: batch:60,epoch:9,loss:1.3658807277679443\n",
      "TEST: batch:70,epoch:9,loss:1.2659510374069214\n",
      "TEST: batch:80,epoch:9,loss:1.4555147886276245\n",
      "TRAIN: batch:50,epoch:10,loss:1.1851747035980225\n",
      "TRAIN: batch:100,epoch:10,loss:1.2233248949050903\n",
      "TRAIN: batch:150,epoch:10,loss:1.2288293838500977\n",
      "TRAIN: batch:200,epoch:10,loss:1.2603175640106201\n",
      "TRAIN: batch:250,epoch:10,loss:1.2359675168991089\n",
      "TRAIN: batch:300,epoch:10,loss:1.2435771226882935\n",
      "TRAIN: batch:350,epoch:10,loss:1.2049413919448853\n",
      "TRAIN: batch:400,epoch:10,loss:1.2660105228424072\n",
      "TRAIN: batch:450,epoch:10,loss:1.2833393812179565\n",
      "TRAIN: batch:500,epoch:10,loss:1.2367738485336304\n",
      "TRAIN: batch:550,epoch:10,loss:1.1415925025939941\n",
      "TRAIN: batch:600,epoch:10,loss:1.2532002925872803\n",
      "TRAIN: batch:650,epoch:10,loss:1.2362034320831299\n",
      "TRAIN: batch:700,epoch:10,loss:1.229358434677124\n",
      "TRAIN: batch:750,epoch:10,loss:1.1956148147583008\n",
      "TEST: batch:10,epoch:10,loss:1.4074456691741943\n",
      "TEST: batch:20,epoch:10,loss:1.4093464612960815\n",
      "TEST: batch:30,epoch:10,loss:1.3771778345108032\n",
      "TEST: batch:40,epoch:10,loss:1.3701483011245728\n",
      "TEST: batch:50,epoch:10,loss:1.3642780780792236\n",
      "TEST: batch:60,epoch:10,loss:1.3455168008804321\n",
      "TEST: batch:70,epoch:10,loss:1.2475963830947876\n",
      "TEST: batch:80,epoch:10,loss:1.445610523223877\n",
      "TRAIN: batch:50,epoch:11,loss:1.166516900062561\n",
      "TRAIN: batch:100,epoch:11,loss:1.207324743270874\n",
      "TRAIN: batch:150,epoch:11,loss:1.207416296005249\n",
      "TRAIN: batch:200,epoch:11,loss:1.238998532295227\n",
      "TRAIN: batch:250,epoch:11,loss:1.2231742143630981\n",
      "TRAIN: batch:300,epoch:11,loss:1.224887728691101\n",
      "TRAIN: batch:350,epoch:11,loss:1.1874749660491943\n",
      "TRAIN: batch:400,epoch:11,loss:1.249692440032959\n",
      "TRAIN: batch:450,epoch:11,loss:1.2565317153930664\n",
      "TRAIN: batch:500,epoch:11,loss:1.2236847877502441\n",
      "TRAIN: batch:550,epoch:11,loss:1.1373646259307861\n",
      "TRAIN: batch:600,epoch:11,loss:1.2418692111968994\n",
      "TRAIN: batch:650,epoch:11,loss:1.2163023948669434\n",
      "TRAIN: batch:700,epoch:11,loss:1.2226239442825317\n",
      "TRAIN: batch:750,epoch:11,loss:1.1712734699249268\n",
      "TEST: batch:10,epoch:11,loss:1.395623803138733\n",
      "TEST: batch:20,epoch:11,loss:1.4067864418029785\n",
      "TEST: batch:30,epoch:11,loss:1.3675293922424316\n",
      "TEST: batch:40,epoch:11,loss:1.3608109951019287\n",
      "TEST: batch:50,epoch:11,loss:1.348683476448059\n",
      "TEST: batch:60,epoch:11,loss:1.3287711143493652\n",
      "TEST: batch:70,epoch:11,loss:1.2627533674240112\n",
      "TEST: batch:80,epoch:11,loss:1.4377126693725586\n",
      "TRAIN: batch:50,epoch:12,loss:1.153076410293579\n",
      "TRAIN: batch:100,epoch:12,loss:1.1969588994979858\n",
      "TRAIN: batch:150,epoch:12,loss:1.1901466846466064\n",
      "TRAIN: batch:200,epoch:12,loss:1.2333556413650513\n",
      "TRAIN: batch:250,epoch:12,loss:1.2039464712142944\n",
      "TRAIN: batch:300,epoch:12,loss:1.2095539569854736\n",
      "TRAIN: batch:350,epoch:12,loss:1.164136290550232\n",
      "TRAIN: batch:400,epoch:12,loss:1.2362637519836426\n",
      "TRAIN: batch:450,epoch:12,loss:1.2616430521011353\n",
      "TRAIN: batch:500,epoch:12,loss:1.2183977365493774\n",
      "TRAIN: batch:550,epoch:12,loss:1.1136195659637451\n",
      "TRAIN: batch:600,epoch:12,loss:1.2338470220565796\n",
      "TRAIN: batch:650,epoch:12,loss:1.201889157295227\n",
      "TRAIN: batch:700,epoch:12,loss:1.2054779529571533\n",
      "TRAIN: batch:750,epoch:12,loss:1.1566163301467896\n",
      "TEST: batch:10,epoch:12,loss:1.3749444484710693\n",
      "TEST: batch:20,epoch:12,loss:1.3904839754104614\n",
      "TEST: batch:30,epoch:12,loss:1.3536142110824585\n",
      "TEST: batch:40,epoch:12,loss:1.359941005706787\n",
      "TEST: batch:50,epoch:12,loss:1.348860502243042\n",
      "TEST: batch:60,epoch:12,loss:1.331567645072937\n",
      "TEST: batch:70,epoch:12,loss:1.2523564100265503\n",
      "TEST: batch:80,epoch:12,loss:1.4216307401657104\n",
      "TRAIN: batch:50,epoch:13,loss:1.136309027671814\n",
      "TRAIN: batch:100,epoch:13,loss:1.1788996458053589\n",
      "TRAIN: batch:150,epoch:13,loss:1.1807868480682373\n",
      "TRAIN: batch:200,epoch:13,loss:1.222299337387085\n",
      "TRAIN: batch:250,epoch:13,loss:1.1925610303878784\n",
      "TRAIN: batch:300,epoch:13,loss:1.199424386024475\n",
      "TRAIN: batch:350,epoch:13,loss:1.1657997369766235\n",
      "TRAIN: batch:400,epoch:13,loss:1.2201920747756958\n",
      "TRAIN: batch:450,epoch:13,loss:1.2259923219680786\n",
      "TRAIN: batch:500,epoch:13,loss:1.1960153579711914\n",
      "TRAIN: batch:550,epoch:13,loss:1.1077542304992676\n",
      "TRAIN: batch:600,epoch:13,loss:1.2099167108535767\n",
      "TRAIN: batch:650,epoch:13,loss:1.2039695978164673\n",
      "TRAIN: batch:700,epoch:13,loss:1.1942678689956665\n",
      "TRAIN: batch:750,epoch:13,loss:1.1406712532043457\n",
      "TEST: batch:10,epoch:13,loss:1.3721528053283691\n",
      "TEST: batch:20,epoch:13,loss:1.3891286849975586\n",
      "TEST: batch:30,epoch:13,loss:1.3394695520401\n",
      "TEST: batch:40,epoch:13,loss:1.3466137647628784\n",
      "TEST: batch:50,epoch:13,loss:1.3339468240737915\n",
      "TEST: batch:60,epoch:13,loss:1.333440899848938\n",
      "TEST: batch:70,epoch:13,loss:1.2456483840942383\n",
      "TEST: batch:80,epoch:13,loss:1.4177844524383545\n",
      "TRAIN: batch:50,epoch:14,loss:1.129173994064331\n",
      "TRAIN: batch:100,epoch:14,loss:1.1719738245010376\n",
      "TRAIN: batch:150,epoch:14,loss:1.1578480005264282\n",
      "TRAIN: batch:200,epoch:14,loss:1.2140777111053467\n",
      "TRAIN: batch:250,epoch:14,loss:1.1797223091125488\n",
      "TRAIN: batch:300,epoch:14,loss:1.1786748170852661\n",
      "TRAIN: batch:350,epoch:14,loss:1.151121735572815\n",
      "TRAIN: batch:400,epoch:14,loss:1.2143213748931885\n",
      "TRAIN: batch:450,epoch:14,loss:1.2204267978668213\n",
      "TRAIN: batch:500,epoch:14,loss:1.1856995820999146\n",
      "TRAIN: batch:550,epoch:14,loss:1.102880835533142\n",
      "TRAIN: batch:600,epoch:14,loss:1.2081855535507202\n",
      "TRAIN: batch:650,epoch:14,loss:1.1779685020446777\n",
      "TRAIN: batch:700,epoch:14,loss:1.1782435178756714\n",
      "TRAIN: batch:750,epoch:14,loss:1.1390774250030518\n",
      "TEST: batch:10,epoch:14,loss:1.3641053438186646\n",
      "TEST: batch:20,epoch:14,loss:1.375889778137207\n",
      "TEST: batch:30,epoch:14,loss:1.3301135301589966\n",
      "TEST: batch:40,epoch:14,loss:1.3450288772583008\n",
      "TEST: batch:50,epoch:14,loss:1.336321234703064\n",
      "TEST: batch:60,epoch:14,loss:1.325117588043213\n",
      "TEST: batch:70,epoch:14,loss:1.2414692640304565\n",
      "TEST: batch:80,epoch:14,loss:1.4209553003311157\n",
      "TRAIN: batch:50,epoch:15,loss:1.1208178997039795\n",
      "TRAIN: batch:100,epoch:15,loss:1.163529634475708\n",
      "TRAIN: batch:150,epoch:15,loss:1.1545159816741943\n",
      "TRAIN: batch:200,epoch:15,loss:1.1994590759277344\n",
      "TRAIN: batch:250,epoch:15,loss:1.1679171323776245\n",
      "TRAIN: batch:300,epoch:15,loss:1.17377507686615\n",
      "TRAIN: batch:350,epoch:15,loss:1.1412543058395386\n",
      "TRAIN: batch:400,epoch:15,loss:1.2020190954208374\n",
      "TRAIN: batch:450,epoch:15,loss:1.2190799713134766\n",
      "TRAIN: batch:500,epoch:15,loss:1.1836936473846436\n",
      "TRAIN: batch:550,epoch:15,loss:1.098150372505188\n",
      "TRAIN: batch:600,epoch:15,loss:1.1849792003631592\n",
      "TRAIN: batch:650,epoch:15,loss:1.1769860982894897\n",
      "TRAIN: batch:700,epoch:15,loss:1.168145775794983\n",
      "TRAIN: batch:750,epoch:15,loss:1.134860873222351\n",
      "TEST: batch:10,epoch:15,loss:1.3595064878463745\n",
      "TEST: batch:20,epoch:15,loss:1.3794610500335693\n",
      "TEST: batch:30,epoch:15,loss:1.331844449043274\n",
      "TEST: batch:40,epoch:15,loss:1.3327618837356567\n",
      "TEST: batch:50,epoch:15,loss:1.3371131420135498\n",
      "TEST: batch:60,epoch:15,loss:1.3105055093765259\n",
      "TEST: batch:70,epoch:15,loss:1.2420192956924438\n",
      "TEST: batch:80,epoch:15,loss:1.4013921022415161\n",
      "TRAIN: batch:50,epoch:16,loss:1.1063528060913086\n",
      "TRAIN: batch:100,epoch:16,loss:1.1484079360961914\n",
      "TRAIN: batch:150,epoch:16,loss:1.1483677625656128\n",
      "TRAIN: batch:200,epoch:16,loss:1.1891671419143677\n",
      "TRAIN: batch:250,epoch:16,loss:1.1614025831222534\n",
      "TRAIN: batch:300,epoch:16,loss:1.1720225811004639\n",
      "TRAIN: batch:350,epoch:16,loss:1.1340129375457764\n",
      "TRAIN: batch:400,epoch:16,loss:1.1919779777526855\n",
      "TRAIN: batch:450,epoch:16,loss:1.2117425203323364\n",
      "TRAIN: batch:500,epoch:16,loss:1.1706863641738892\n",
      "TRAIN: batch:550,epoch:16,loss:1.0871353149414062\n",
      "TRAIN: batch:600,epoch:16,loss:1.1802235841751099\n",
      "TRAIN: batch:650,epoch:16,loss:1.1676647663116455\n",
      "TRAIN: batch:700,epoch:16,loss:1.1670825481414795\n",
      "TRAIN: batch:750,epoch:16,loss:1.1248773336410522\n",
      "TEST: batch:10,epoch:16,loss:1.3588157892227173\n",
      "TEST: batch:20,epoch:16,loss:1.3649824857711792\n",
      "TEST: batch:30,epoch:16,loss:1.334153413772583\n",
      "TEST: batch:40,epoch:16,loss:1.3280295133590698\n",
      "TEST: batch:50,epoch:16,loss:1.3316630125045776\n",
      "TEST: batch:60,epoch:16,loss:1.313593864440918\n",
      "TEST: batch:70,epoch:16,loss:1.2202157974243164\n",
      "TEST: batch:80,epoch:16,loss:1.3987798690795898\n",
      "TRAIN: batch:50,epoch:17,loss:1.1035029888153076\n",
      "TRAIN: batch:100,epoch:17,loss:1.1474684476852417\n",
      "TRAIN: batch:150,epoch:17,loss:1.1374425888061523\n",
      "TRAIN: batch:200,epoch:17,loss:1.1758794784545898\n",
      "TRAIN: batch:250,epoch:17,loss:1.160883903503418\n",
      "TRAIN: batch:300,epoch:17,loss:1.1637707948684692\n",
      "TRAIN: batch:350,epoch:17,loss:1.1265532970428467\n",
      "TRAIN: batch:400,epoch:17,loss:1.184993028640747\n",
      "TRAIN: batch:450,epoch:17,loss:1.2068148851394653\n",
      "TRAIN: batch:500,epoch:17,loss:1.1650465726852417\n",
      "TRAIN: batch:550,epoch:17,loss:1.0792741775512695\n",
      "TRAIN: batch:600,epoch:17,loss:1.1859865188598633\n",
      "TRAIN: batch:650,epoch:17,loss:1.1601029634475708\n",
      "TRAIN: batch:700,epoch:17,loss:1.1487138271331787\n",
      "TRAIN: batch:750,epoch:17,loss:1.12522292137146\n",
      "TEST: batch:10,epoch:17,loss:1.3614169359207153\n",
      "TEST: batch:20,epoch:17,loss:1.3742996454238892\n",
      "TEST: batch:30,epoch:17,loss:1.3248051404953003\n",
      "TEST: batch:40,epoch:17,loss:1.3242765665054321\n",
      "TEST: batch:50,epoch:17,loss:1.3146902322769165\n",
      "TEST: batch:60,epoch:17,loss:1.3061085939407349\n",
      "TEST: batch:70,epoch:17,loss:1.229576826095581\n",
      "TEST: batch:80,epoch:17,loss:1.4016743898391724\n",
      "TRAIN: batch:50,epoch:18,loss:1.0909955501556396\n",
      "TRAIN: batch:100,epoch:18,loss:1.1343125104904175\n",
      "TRAIN: batch:150,epoch:18,loss:1.1343858242034912\n",
      "TRAIN: batch:200,epoch:18,loss:1.1864089965820312\n",
      "TRAIN: batch:250,epoch:18,loss:1.1401300430297852\n",
      "TRAIN: batch:300,epoch:18,loss:1.1595900058746338\n",
      "TRAIN: batch:350,epoch:18,loss:1.1089487075805664\n",
      "TRAIN: batch:400,epoch:18,loss:1.185191035270691\n",
      "TRAIN: batch:450,epoch:18,loss:1.1941081285476685\n",
      "TRAIN: batch:500,epoch:18,loss:1.1593480110168457\n",
      "TRAIN: batch:550,epoch:18,loss:1.0688891410827637\n",
      "TRAIN: batch:600,epoch:18,loss:1.1700392961502075\n",
      "TRAIN: batch:650,epoch:18,loss:1.1561793088912964\n",
      "TRAIN: batch:700,epoch:18,loss:1.153160810470581\n",
      "TRAIN: batch:750,epoch:18,loss:1.1032609939575195\n",
      "TEST: batch:10,epoch:18,loss:1.3552768230438232\n",
      "TEST: batch:20,epoch:18,loss:1.3739036321640015\n",
      "TEST: batch:30,epoch:18,loss:1.3343600034713745\n",
      "TEST: batch:40,epoch:18,loss:1.3298048973083496\n",
      "TEST: batch:50,epoch:18,loss:1.3258464336395264\n",
      "TEST: batch:60,epoch:18,loss:1.303709864616394\n",
      "TEST: batch:70,epoch:18,loss:1.2231804132461548\n",
      "TEST: batch:80,epoch:18,loss:1.4044967889785767\n",
      "TRAIN: batch:50,epoch:19,loss:1.0926874876022339\n",
      "TRAIN: batch:100,epoch:19,loss:1.1226474046707153\n",
      "TRAIN: batch:150,epoch:19,loss:1.1280453205108643\n",
      "TRAIN: batch:200,epoch:19,loss:1.166557788848877\n",
      "TRAIN: batch:250,epoch:19,loss:1.1347101926803589\n",
      "TRAIN: batch:300,epoch:19,loss:1.1567909717559814\n",
      "TRAIN: batch:350,epoch:19,loss:1.100060224533081\n",
      "TRAIN: batch:400,epoch:19,loss:1.1638824939727783\n",
      "TRAIN: batch:450,epoch:19,loss:1.1901402473449707\n",
      "TRAIN: batch:500,epoch:19,loss:1.145156741142273\n",
      "TRAIN: batch:550,epoch:19,loss:1.0651556253433228\n",
      "TRAIN: batch:600,epoch:19,loss:1.172295093536377\n",
      "TRAIN: batch:650,epoch:19,loss:1.152004599571228\n",
      "TRAIN: batch:700,epoch:19,loss:1.1389249563217163\n",
      "TRAIN: batch:750,epoch:19,loss:1.1037524938583374\n",
      "TEST: batch:10,epoch:19,loss:1.3475079536437988\n",
      "TEST: batch:20,epoch:19,loss:1.3837891817092896\n",
      "TEST: batch:30,epoch:19,loss:1.3334046602249146\n",
      "TEST: batch:40,epoch:19,loss:1.3354606628417969\n",
      "TEST: batch:50,epoch:19,loss:1.3263756036758423\n",
      "TEST: batch:60,epoch:19,loss:1.2998508214950562\n",
      "TEST: batch:70,epoch:19,loss:1.2181711196899414\n",
      "TEST: batch:80,epoch:19,loss:1.3951071500778198\n",
      "TRAIN: batch:50,epoch:20,loss:1.0874994993209839\n",
      "TRAIN: batch:100,epoch:20,loss:1.1275248527526855\n",
      "TRAIN: batch:150,epoch:20,loss:1.1270431280136108\n",
      "TRAIN: batch:200,epoch:20,loss:1.1667838096618652\n",
      "TRAIN: batch:250,epoch:20,loss:1.130990982055664\n",
      "TRAIN: batch:300,epoch:20,loss:1.1399153470993042\n",
      "TRAIN: batch:350,epoch:20,loss:1.0978960990905762\n",
      "TRAIN: batch:400,epoch:20,loss:1.1722309589385986\n",
      "TRAIN: batch:450,epoch:20,loss:1.1763341426849365\n",
      "TRAIN: batch:500,epoch:20,loss:1.149357557296753\n",
      "TRAIN: batch:550,epoch:20,loss:1.0619757175445557\n",
      "TRAIN: batch:600,epoch:20,loss:1.1693228483200073\n",
      "TRAIN: batch:650,epoch:20,loss:1.1396158933639526\n",
      "TRAIN: batch:700,epoch:20,loss:1.1368992328643799\n",
      "TRAIN: batch:750,epoch:20,loss:1.1018147468566895\n",
      "TEST: batch:10,epoch:20,loss:1.354914903640747\n",
      "TEST: batch:20,epoch:20,loss:1.3490967750549316\n",
      "TEST: batch:30,epoch:20,loss:1.3149209022521973\n",
      "TEST: batch:40,epoch:20,loss:1.3161845207214355\n",
      "TEST: batch:50,epoch:20,loss:1.3098071813583374\n",
      "TEST: batch:60,epoch:20,loss:1.2928792238235474\n",
      "TEST: batch:70,epoch:20,loss:1.2248097658157349\n",
      "TEST: batch:80,epoch:20,loss:1.4131261110305786\n",
      "TRAIN: batch:50,epoch:21,loss:1.0820850133895874\n",
      "TRAIN: batch:100,epoch:21,loss:1.1171847581863403\n",
      "TRAIN: batch:150,epoch:21,loss:1.1095528602600098\n",
      "TRAIN: batch:200,epoch:21,loss:1.1595282554626465\n",
      "TRAIN: batch:250,epoch:21,loss:1.130787968635559\n",
      "TRAIN: batch:300,epoch:21,loss:1.137280821800232\n",
      "TRAIN: batch:350,epoch:21,loss:1.105270266532898\n",
      "TRAIN: batch:400,epoch:21,loss:1.1608635187149048\n",
      "TRAIN: batch:450,epoch:21,loss:1.186276912689209\n",
      "TRAIN: batch:500,epoch:21,loss:1.1479980945587158\n",
      "TRAIN: batch:550,epoch:21,loss:1.0555155277252197\n",
      "TRAIN: batch:600,epoch:21,loss:1.1627799272537231\n",
      "TRAIN: batch:650,epoch:21,loss:1.1344513893127441\n",
      "TRAIN: batch:700,epoch:21,loss:1.132615566253662\n",
      "TRAIN: batch:750,epoch:21,loss:1.0927629470825195\n",
      "TEST: batch:10,epoch:21,loss:1.3460637331008911\n",
      "TEST: batch:20,epoch:21,loss:1.371931552886963\n",
      "TEST: batch:30,epoch:21,loss:1.3201571702957153\n",
      "TEST: batch:40,epoch:21,loss:1.3074727058410645\n",
      "TEST: batch:50,epoch:21,loss:1.30608332157135\n",
      "TEST: batch:60,epoch:21,loss:1.2992945909500122\n",
      "TEST: batch:70,epoch:21,loss:1.2164037227630615\n",
      "TEST: batch:80,epoch:21,loss:1.3915164470672607\n",
      "TRAIN: batch:50,epoch:22,loss:1.0755221843719482\n",
      "TRAIN: batch:100,epoch:22,loss:1.1109821796417236\n",
      "TRAIN: batch:150,epoch:22,loss:1.11159348487854\n",
      "TRAIN: batch:200,epoch:22,loss:1.1515381336212158\n",
      "TRAIN: batch:250,epoch:22,loss:1.121383786201477\n",
      "TRAIN: batch:300,epoch:22,loss:1.1396129131317139\n",
      "TRAIN: batch:350,epoch:22,loss:1.0885584354400635\n",
      "TRAIN: batch:400,epoch:22,loss:1.1603509187698364\n",
      "TRAIN: batch:450,epoch:22,loss:1.1688663959503174\n",
      "TRAIN: batch:500,epoch:22,loss:1.131561279296875\n",
      "TRAIN: batch:550,epoch:22,loss:1.0474435091018677\n",
      "TRAIN: batch:600,epoch:22,loss:1.1530330181121826\n",
      "TRAIN: batch:650,epoch:22,loss:1.1349530220031738\n",
      "TRAIN: batch:700,epoch:22,loss:1.126570701599121\n",
      "TRAIN: batch:750,epoch:22,loss:1.091610312461853\n",
      "TEST: batch:10,epoch:22,loss:1.3424086570739746\n",
      "TEST: batch:20,epoch:22,loss:1.353071689605713\n",
      "TEST: batch:30,epoch:22,loss:1.3196557760238647\n",
      "TEST: batch:40,epoch:22,loss:1.33021879196167\n",
      "TEST: batch:50,epoch:22,loss:1.3262523412704468\n",
      "TEST: batch:60,epoch:22,loss:1.3061763048171997\n",
      "TEST: batch:70,epoch:22,loss:1.2222083806991577\n",
      "TEST: batch:80,epoch:22,loss:1.3946278095245361\n",
      "TRAIN: batch:50,epoch:23,loss:1.060347080230713\n",
      "TRAIN: batch:100,epoch:23,loss:1.1082261800765991\n",
      "TRAIN: batch:150,epoch:23,loss:1.1098406314849854\n",
      "TRAIN: batch:200,epoch:23,loss:1.1456141471862793\n",
      "TRAIN: batch:250,epoch:23,loss:1.1108490228652954\n",
      "TRAIN: batch:300,epoch:23,loss:1.1221890449523926\n",
      "TRAIN: batch:350,epoch:23,loss:1.091739535331726\n",
      "TRAIN: batch:400,epoch:23,loss:1.1469032764434814\n",
      "TRAIN: batch:450,epoch:23,loss:1.1644587516784668\n",
      "TRAIN: batch:500,epoch:23,loss:1.131856918334961\n",
      "TRAIN: batch:550,epoch:23,loss:1.0488032102584839\n",
      "TRAIN: batch:600,epoch:23,loss:1.1432753801345825\n",
      "TRAIN: batch:650,epoch:23,loss:1.1263070106506348\n",
      "TRAIN: batch:700,epoch:23,loss:1.123260736465454\n",
      "TRAIN: batch:750,epoch:23,loss:1.0845640897750854\n",
      "TEST: batch:10,epoch:23,loss:1.3473994731903076\n",
      "TEST: batch:20,epoch:23,loss:1.370192289352417\n",
      "TEST: batch:30,epoch:23,loss:1.3161919116973877\n",
      "TEST: batch:40,epoch:23,loss:1.320265293121338\n",
      "TEST: batch:50,epoch:23,loss:1.320522427558899\n",
      "TEST: batch:60,epoch:23,loss:1.296830654144287\n",
      "TEST: batch:70,epoch:23,loss:1.223435401916504\n",
      "TEST: batch:80,epoch:23,loss:1.3939824104309082\n",
      "TRAIN: batch:50,epoch:24,loss:1.065177083015442\n",
      "TRAIN: batch:100,epoch:24,loss:1.1058964729309082\n",
      "TRAIN: batch:150,epoch:24,loss:1.097875714302063\n",
      "TRAIN: batch:200,epoch:24,loss:1.1534408330917358\n",
      "TRAIN: batch:250,epoch:24,loss:1.1166869401931763\n",
      "TRAIN: batch:300,epoch:24,loss:1.127274990081787\n",
      "TRAIN: batch:350,epoch:24,loss:1.0946290493011475\n",
      "TRAIN: batch:400,epoch:24,loss:1.1344505548477173\n",
      "TRAIN: batch:450,epoch:24,loss:1.1605859994888306\n",
      "TRAIN: batch:500,epoch:24,loss:1.1208728551864624\n",
      "TRAIN: batch:550,epoch:24,loss:1.039900302886963\n",
      "TRAIN: batch:600,epoch:24,loss:1.1533327102661133\n",
      "TRAIN: batch:650,epoch:24,loss:1.129600167274475\n",
      "TRAIN: batch:700,epoch:24,loss:1.112299919128418\n",
      "TRAIN: batch:750,epoch:24,loss:1.080443024635315\n",
      "TEST: batch:10,epoch:24,loss:1.343916654586792\n",
      "TEST: batch:20,epoch:24,loss:1.3674206733703613\n",
      "TEST: batch:30,epoch:24,loss:1.3055474758148193\n",
      "TEST: batch:40,epoch:24,loss:1.3195527791976929\n",
      "TEST: batch:50,epoch:24,loss:1.3129539489746094\n",
      "TEST: batch:60,epoch:24,loss:1.2921843528747559\n",
      "TEST: batch:70,epoch:24,loss:1.217881679534912\n",
      "TEST: batch:80,epoch:24,loss:1.3941608667373657\n",
      "TRAIN: batch:50,epoch:25,loss:1.0563734769821167\n",
      "TRAIN: batch:100,epoch:25,loss:1.0953521728515625\n",
      "TRAIN: batch:150,epoch:25,loss:1.0977718830108643\n",
      "TRAIN: batch:200,epoch:25,loss:1.1542441844940186\n",
      "TRAIN: batch:250,epoch:25,loss:1.1078650951385498\n",
      "TRAIN: batch:300,epoch:25,loss:1.1109732389450073\n",
      "TRAIN: batch:350,epoch:25,loss:1.0785216093063354\n",
      "TRAIN: batch:400,epoch:25,loss:1.134037971496582\n",
      "TRAIN: batch:450,epoch:25,loss:1.1695150136947632\n",
      "TRAIN: batch:500,epoch:25,loss:1.1191112995147705\n",
      "TRAIN: batch:550,epoch:25,loss:1.0441287755966187\n",
      "TRAIN: batch:600,epoch:25,loss:1.1360057592391968\n",
      "TRAIN: batch:650,epoch:25,loss:1.1228419542312622\n",
      "TRAIN: batch:700,epoch:25,loss:1.1080923080444336\n",
      "TRAIN: batch:750,epoch:25,loss:1.0675090551376343\n",
      "TEST: batch:10,epoch:25,loss:1.3384562730789185\n",
      "TEST: batch:20,epoch:25,loss:1.3655880689620972\n",
      "TEST: batch:30,epoch:25,loss:1.3166799545288086\n",
      "TEST: batch:40,epoch:25,loss:1.3251376152038574\n",
      "TEST: batch:50,epoch:25,loss:1.3137977123260498\n",
      "TEST: batch:60,epoch:25,loss:1.3055397272109985\n",
      "TEST: batch:70,epoch:25,loss:1.2067527770996094\n",
      "TEST: batch:80,epoch:25,loss:1.4063653945922852\n",
      "TRAIN: batch:50,epoch:26,loss:1.0550487041473389\n",
      "TRAIN: batch:100,epoch:26,loss:1.092786431312561\n",
      "TRAIN: batch:150,epoch:26,loss:1.0925719738006592\n",
      "TRAIN: batch:200,epoch:26,loss:1.1399153470993042\n",
      "TRAIN: batch:250,epoch:26,loss:1.0998449325561523\n",
      "TRAIN: batch:300,epoch:26,loss:1.1211878061294556\n",
      "TRAIN: batch:350,epoch:26,loss:1.0770665407180786\n",
      "TRAIN: batch:400,epoch:26,loss:1.1296298503875732\n",
      "TRAIN: batch:450,epoch:26,loss:1.1537705659866333\n",
      "TRAIN: batch:500,epoch:26,loss:1.1179494857788086\n",
      "TRAIN: batch:550,epoch:26,loss:1.0377793312072754\n",
      "TRAIN: batch:600,epoch:26,loss:1.130913257598877\n",
      "TRAIN: batch:650,epoch:26,loss:1.1244425773620605\n",
      "TRAIN: batch:700,epoch:26,loss:1.103117823600769\n",
      "TRAIN: batch:750,epoch:26,loss:1.0684820413589478\n",
      "TEST: batch:10,epoch:26,loss:1.3419435024261475\n",
      "TEST: batch:20,epoch:26,loss:1.35590398311615\n",
      "TEST: batch:30,epoch:26,loss:1.3021271228790283\n",
      "TEST: batch:40,epoch:26,loss:1.3096413612365723\n",
      "TEST: batch:50,epoch:26,loss:1.3036483526229858\n",
      "TEST: batch:60,epoch:26,loss:1.3020007610321045\n",
      "TEST: batch:70,epoch:26,loss:1.2093236446380615\n",
      "TEST: batch:80,epoch:26,loss:1.3815984725952148\n",
      "TRAIN: batch:50,epoch:27,loss:1.0599372386932373\n",
      "TRAIN: batch:100,epoch:27,loss:1.0981582403182983\n",
      "TRAIN: batch:150,epoch:27,loss:1.0917134284973145\n",
      "TRAIN: batch:200,epoch:27,loss:1.1379352807998657\n",
      "TRAIN: batch:250,epoch:27,loss:1.0895668268203735\n",
      "TRAIN: batch:300,epoch:27,loss:1.1187328100204468\n",
      "TRAIN: batch:350,epoch:27,loss:1.0781559944152832\n",
      "TRAIN: batch:400,epoch:27,loss:1.1389471292495728\n",
      "TRAIN: batch:450,epoch:27,loss:1.1497465372085571\n",
      "TRAIN: batch:500,epoch:27,loss:1.1098209619522095\n",
      "TRAIN: batch:550,epoch:27,loss:1.028841495513916\n",
      "TRAIN: batch:600,epoch:27,loss:1.1331027746200562\n",
      "TRAIN: batch:650,epoch:27,loss:1.1194132566452026\n",
      "TRAIN: batch:700,epoch:27,loss:1.108614444732666\n",
      "TRAIN: batch:750,epoch:27,loss:1.0717709064483643\n",
      "TEST: batch:10,epoch:27,loss:1.3395973443984985\n",
      "TEST: batch:20,epoch:27,loss:1.3516746759414673\n",
      "TEST: batch:30,epoch:27,loss:1.2877665758132935\n",
      "TEST: batch:40,epoch:27,loss:1.3239060640335083\n",
      "TEST: batch:50,epoch:27,loss:1.3042560815811157\n",
      "TEST: batch:60,epoch:27,loss:1.2915258407592773\n",
      "TEST: batch:70,epoch:27,loss:1.2126623392105103\n",
      "TEST: batch:80,epoch:27,loss:1.3870532512664795\n",
      "TRAIN: batch:50,epoch:28,loss:1.056700348854065\n",
      "TRAIN: batch:100,epoch:28,loss:1.0820614099502563\n",
      "TRAIN: batch:150,epoch:28,loss:1.0801900625228882\n",
      "TRAIN: batch:200,epoch:28,loss:1.1233489513397217\n",
      "TRAIN: batch:250,epoch:28,loss:1.0943571329116821\n",
      "TRAIN: batch:300,epoch:28,loss:1.0935214757919312\n",
      "TRAIN: batch:350,epoch:28,loss:1.0763715505599976\n",
      "TRAIN: batch:400,epoch:28,loss:1.1304255723953247\n",
      "TRAIN: batch:450,epoch:28,loss:1.1468420028686523\n",
      "TRAIN: batch:500,epoch:28,loss:1.1071269512176514\n",
      "TRAIN: batch:550,epoch:28,loss:1.0242840051651\n",
      "TRAIN: batch:600,epoch:28,loss:1.136887788772583\n",
      "TRAIN: batch:650,epoch:28,loss:1.109452724456787\n",
      "TRAIN: batch:700,epoch:28,loss:1.1000621318817139\n",
      "TRAIN: batch:750,epoch:28,loss:1.0665217638015747\n",
      "TEST: batch:10,epoch:28,loss:1.3284199237823486\n",
      "TEST: batch:20,epoch:28,loss:1.3662837743759155\n",
      "TEST: batch:30,epoch:28,loss:1.3015227317810059\n",
      "TEST: batch:40,epoch:28,loss:1.3159452676773071\n",
      "TEST: batch:50,epoch:28,loss:1.3117098808288574\n",
      "TEST: batch:60,epoch:28,loss:1.2847638130187988\n",
      "TEST: batch:70,epoch:28,loss:1.2135515213012695\n",
      "TEST: batch:80,epoch:28,loss:1.3867449760437012\n",
      "TRAIN: batch:50,epoch:29,loss:1.0665390491485596\n",
      "TRAIN: batch:100,epoch:29,loss:1.0852794647216797\n",
      "TRAIN: batch:150,epoch:29,loss:1.080559253692627\n",
      "TRAIN: batch:200,epoch:29,loss:1.1310491561889648\n",
      "TRAIN: batch:250,epoch:29,loss:1.0892813205718994\n",
      "TRAIN: batch:300,epoch:29,loss:1.1074837446212769\n",
      "TRAIN: batch:350,epoch:29,loss:1.0673795938491821\n",
      "TRAIN: batch:400,epoch:29,loss:1.1288889646530151\n",
      "TRAIN: batch:450,epoch:29,loss:1.13943612575531\n",
      "TRAIN: batch:500,epoch:29,loss:1.1088579893112183\n",
      "TRAIN: batch:550,epoch:29,loss:1.0163631439208984\n",
      "TRAIN: batch:600,epoch:29,loss:1.126189112663269\n",
      "TRAIN: batch:650,epoch:29,loss:1.1136527061462402\n",
      "TRAIN: batch:700,epoch:29,loss:1.097038984298706\n",
      "TRAIN: batch:750,epoch:29,loss:1.0641014575958252\n",
      "TEST: batch:10,epoch:29,loss:1.3237922191619873\n",
      "TEST: batch:20,epoch:29,loss:1.3469702005386353\n",
      "TEST: batch:30,epoch:29,loss:1.300520658493042\n",
      "TEST: batch:40,epoch:29,loss:1.3179606199264526\n",
      "TEST: batch:50,epoch:29,loss:1.2883435487747192\n",
      "TEST: batch:60,epoch:29,loss:1.2944084405899048\n",
      "TEST: batch:70,epoch:29,loss:1.2125482559204102\n",
      "TEST: batch:80,epoch:29,loss:1.39042067527771\n",
      "TRAIN: batch:50,epoch:30,loss:1.0472878217697144\n",
      "TRAIN: batch:100,epoch:30,loss:1.0735132694244385\n",
      "TRAIN: batch:150,epoch:30,loss:1.079673409461975\n",
      "TRAIN: batch:200,epoch:30,loss:1.1198290586471558\n",
      "TRAIN: batch:250,epoch:30,loss:1.087977647781372\n",
      "TRAIN: batch:300,epoch:30,loss:1.1025470495224\n",
      "TRAIN: batch:350,epoch:30,loss:1.0574626922607422\n",
      "TRAIN: batch:400,epoch:30,loss:1.1186021566390991\n",
      "TRAIN: batch:450,epoch:30,loss:1.1408628225326538\n",
      "TRAIN: batch:500,epoch:30,loss:1.1047042608261108\n",
      "TRAIN: batch:550,epoch:30,loss:1.017551064491272\n",
      "TRAIN: batch:600,epoch:30,loss:1.135428547859192\n",
      "TRAIN: batch:650,epoch:30,loss:1.0998377799987793\n",
      "TRAIN: batch:700,epoch:30,loss:1.0960689783096313\n",
      "TRAIN: batch:750,epoch:30,loss:1.065494418144226\n",
      "TEST: batch:10,epoch:30,loss:1.3324332237243652\n",
      "TEST: batch:20,epoch:30,loss:1.3575868606567383\n",
      "TEST: batch:30,epoch:30,loss:1.304499864578247\n",
      "TEST: batch:40,epoch:30,loss:1.31746244430542\n",
      "TEST: batch:50,epoch:30,loss:1.3019158840179443\n",
      "TEST: batch:60,epoch:30,loss:1.2853847742080688\n",
      "TEST: batch:70,epoch:30,loss:1.2049574851989746\n",
      "TEST: batch:80,epoch:30,loss:1.3735586404800415\n",
      "TRAIN: batch:50,epoch:31,loss:1.044803261756897\n",
      "TRAIN: batch:100,epoch:31,loss:1.0676746368408203\n",
      "TRAIN: batch:150,epoch:31,loss:1.0743515491485596\n",
      "TRAIN: batch:200,epoch:31,loss:1.128774881362915\n",
      "TRAIN: batch:250,epoch:31,loss:1.0769964456558228\n",
      "TRAIN: batch:300,epoch:31,loss:1.1030511856079102\n",
      "TRAIN: batch:350,epoch:31,loss:1.0597147941589355\n",
      "TRAIN: batch:400,epoch:31,loss:1.1209686994552612\n",
      "TRAIN: batch:450,epoch:31,loss:1.1493550539016724\n",
      "TRAIN: batch:500,epoch:31,loss:1.0923212766647339\n",
      "TRAIN: batch:550,epoch:31,loss:1.0142743587493896\n",
      "TRAIN: batch:600,epoch:31,loss:1.119439959526062\n",
      "TRAIN: batch:650,epoch:31,loss:1.09788179397583\n",
      "TRAIN: batch:700,epoch:31,loss:1.086103081703186\n",
      "TRAIN: batch:750,epoch:31,loss:1.0502060651779175\n",
      "TEST: batch:10,epoch:31,loss:1.3510109186172485\n",
      "TEST: batch:20,epoch:31,loss:1.3414448499679565\n",
      "TEST: batch:30,epoch:31,loss:1.2952831983566284\n",
      "TEST: batch:40,epoch:31,loss:1.3142752647399902\n",
      "TEST: batch:50,epoch:31,loss:1.2912170886993408\n",
      "TEST: batch:60,epoch:31,loss:1.2903413772583008\n",
      "TEST: batch:70,epoch:31,loss:1.2031123638153076\n",
      "TEST: batch:80,epoch:31,loss:1.3918671607971191\n",
      "TRAIN: batch:50,epoch:32,loss:1.0479176044464111\n",
      "TRAIN: batch:100,epoch:32,loss:1.0698020458221436\n",
      "TRAIN: batch:150,epoch:32,loss:1.067763328552246\n",
      "TRAIN: batch:200,epoch:32,loss:1.1155977249145508\n",
      "TRAIN: batch:250,epoch:32,loss:1.0743352174758911\n",
      "TRAIN: batch:300,epoch:32,loss:1.0897845029830933\n",
      "TRAIN: batch:350,epoch:32,loss:1.0666497945785522\n",
      "TRAIN: batch:400,epoch:32,loss:1.1053868532180786\n",
      "TRAIN: batch:450,epoch:32,loss:1.1239221096038818\n",
      "TRAIN: batch:500,epoch:32,loss:1.0903476476669312\n",
      "TRAIN: batch:550,epoch:32,loss:1.0168532133102417\n",
      "TRAIN: batch:600,epoch:32,loss:1.112262487411499\n",
      "TRAIN: batch:650,epoch:32,loss:1.0958149433135986\n",
      "TRAIN: batch:700,epoch:32,loss:1.0853729248046875\n",
      "TRAIN: batch:750,epoch:32,loss:1.0422419309616089\n",
      "TEST: batch:10,epoch:32,loss:1.3181557655334473\n",
      "TEST: batch:20,epoch:32,loss:1.3666610717773438\n",
      "TEST: batch:30,epoch:32,loss:1.2956608533859253\n",
      "TEST: batch:40,epoch:32,loss:1.3175314664840698\n",
      "TEST: batch:50,epoch:32,loss:1.3025811910629272\n",
      "TEST: batch:60,epoch:32,loss:1.2762001752853394\n",
      "TEST: batch:70,epoch:32,loss:1.2005616426467896\n",
      "TEST: batch:80,epoch:32,loss:1.4040231704711914\n",
      "TRAIN: batch:50,epoch:33,loss:1.0354276895523071\n",
      "TRAIN: batch:100,epoch:33,loss:1.0661187171936035\n",
      "TRAIN: batch:150,epoch:33,loss:1.080998182296753\n",
      "TRAIN: batch:200,epoch:33,loss:1.115789771080017\n",
      "TRAIN: batch:250,epoch:33,loss:1.0754971504211426\n",
      "TRAIN: batch:300,epoch:33,loss:1.0924720764160156\n",
      "TRAIN: batch:350,epoch:33,loss:1.057545781135559\n",
      "TRAIN: batch:400,epoch:33,loss:1.1092727184295654\n",
      "TRAIN: batch:450,epoch:33,loss:1.1235793828964233\n",
      "TRAIN: batch:500,epoch:33,loss:1.0858463048934937\n",
      "TRAIN: batch:550,epoch:33,loss:1.0165702104568481\n",
      "TRAIN: batch:600,epoch:33,loss:1.1171023845672607\n",
      "TRAIN: batch:650,epoch:33,loss:1.092650294303894\n",
      "TRAIN: batch:700,epoch:33,loss:1.088751196861267\n",
      "TRAIN: batch:750,epoch:33,loss:1.0437930822372437\n",
      "TEST: batch:10,epoch:33,loss:1.3290005922317505\n",
      "TEST: batch:20,epoch:33,loss:1.357419729232788\n",
      "TEST: batch:30,epoch:33,loss:1.294937252998352\n",
      "TEST: batch:40,epoch:33,loss:1.3042094707489014\n",
      "TEST: batch:50,epoch:33,loss:1.3039435148239136\n",
      "TEST: batch:60,epoch:33,loss:1.2923529148101807\n",
      "TEST: batch:70,epoch:33,loss:1.2076549530029297\n",
      "TEST: batch:80,epoch:33,loss:1.3907721042633057\n",
      "TRAIN: batch:50,epoch:34,loss:1.0282509326934814\n",
      "TRAIN: batch:100,epoch:34,loss:1.0596439838409424\n",
      "TRAIN: batch:150,epoch:34,loss:1.0623279809951782\n",
      "TRAIN: batch:200,epoch:34,loss:1.1069096326828003\n",
      "TRAIN: batch:250,epoch:34,loss:1.0743179321289062\n",
      "TRAIN: batch:300,epoch:34,loss:1.0874876976013184\n",
      "TRAIN: batch:350,epoch:34,loss:1.0451496839523315\n",
      "TRAIN: batch:400,epoch:34,loss:1.1048704385757446\n",
      "TRAIN: batch:450,epoch:34,loss:1.1316571235656738\n",
      "TRAIN: batch:500,epoch:34,loss:1.0925010442733765\n",
      "TRAIN: batch:550,epoch:34,loss:1.003048062324524\n",
      "TRAIN: batch:600,epoch:34,loss:1.103359580039978\n",
      "TRAIN: batch:650,epoch:34,loss:1.0908479690551758\n",
      "TRAIN: batch:700,epoch:34,loss:1.0808348655700684\n",
      "TRAIN: batch:750,epoch:34,loss:1.0421675443649292\n",
      "TEST: batch:10,epoch:34,loss:1.3169306516647339\n",
      "TEST: batch:20,epoch:34,loss:1.3491212129592896\n",
      "TEST: batch:30,epoch:34,loss:1.2844429016113281\n",
      "TEST: batch:40,epoch:34,loss:1.311066746711731\n",
      "TEST: batch:50,epoch:34,loss:1.2983016967773438\n",
      "TEST: batch:60,epoch:34,loss:1.2806075811386108\n",
      "TEST: batch:70,epoch:34,loss:1.2072796821594238\n",
      "TEST: batch:80,epoch:34,loss:1.3850245475769043\n",
      "TRAIN: batch:50,epoch:35,loss:1.0281254053115845\n",
      "TRAIN: batch:100,epoch:35,loss:1.0661126375198364\n",
      "TRAIN: batch:150,epoch:35,loss:1.049614667892456\n",
      "TRAIN: batch:200,epoch:35,loss:1.1046664714813232\n",
      "TRAIN: batch:250,epoch:35,loss:1.0623279809951782\n",
      "TRAIN: batch:300,epoch:35,loss:1.0837132930755615\n",
      "TRAIN: batch:350,epoch:35,loss:1.0462281703948975\n",
      "TRAIN: batch:400,epoch:35,loss:1.096221685409546\n",
      "TRAIN: batch:450,epoch:35,loss:1.1211858987808228\n",
      "TRAIN: batch:500,epoch:35,loss:1.09027898311615\n",
      "TRAIN: batch:550,epoch:35,loss:1.004429817199707\n",
      "TRAIN: batch:600,epoch:35,loss:1.1026712656021118\n",
      "TRAIN: batch:650,epoch:35,loss:1.0890531539916992\n",
      "TRAIN: batch:700,epoch:35,loss:1.074398159980774\n",
      "TRAIN: batch:750,epoch:35,loss:1.037510633468628\n",
      "TEST: batch:10,epoch:35,loss:1.3222596645355225\n",
      "TEST: batch:20,epoch:35,loss:1.3549352884292603\n",
      "TEST: batch:30,epoch:35,loss:1.2864296436309814\n",
      "TEST: batch:40,epoch:35,loss:1.3276978731155396\n",
      "TEST: batch:50,epoch:35,loss:1.3205276727676392\n",
      "TEST: batch:60,epoch:35,loss:1.2778054475784302\n",
      "TEST: batch:70,epoch:35,loss:1.2081193923950195\n",
      "TEST: batch:80,epoch:35,loss:1.3826243877410889\n",
      "TRAIN: batch:50,epoch:36,loss:1.0238251686096191\n",
      "TRAIN: batch:100,epoch:36,loss:1.0602936744689941\n",
      "TRAIN: batch:150,epoch:36,loss:1.0573173761367798\n",
      "TRAIN: batch:200,epoch:36,loss:1.115770936012268\n",
      "TRAIN: batch:250,epoch:36,loss:1.0693145990371704\n",
      "TRAIN: batch:300,epoch:36,loss:1.0840115547180176\n",
      "TRAIN: batch:350,epoch:36,loss:1.050809383392334\n",
      "TRAIN: batch:400,epoch:36,loss:1.1112544536590576\n",
      "TRAIN: batch:450,epoch:36,loss:1.1201815605163574\n",
      "TRAIN: batch:500,epoch:36,loss:1.0828171968460083\n",
      "TRAIN: batch:550,epoch:36,loss:1.008560299873352\n",
      "TRAIN: batch:600,epoch:36,loss:1.112429141998291\n",
      "TRAIN: batch:650,epoch:36,loss:1.0911104679107666\n",
      "TRAIN: batch:700,epoch:36,loss:1.0767234563827515\n",
      "TRAIN: batch:750,epoch:36,loss:1.0333662033081055\n",
      "TEST: batch:10,epoch:36,loss:1.334280252456665\n",
      "TEST: batch:20,epoch:36,loss:1.3593617677688599\n",
      "TEST: batch:30,epoch:36,loss:1.2854100465774536\n",
      "TEST: batch:40,epoch:36,loss:1.3243615627288818\n",
      "TEST: batch:50,epoch:36,loss:1.309990406036377\n",
      "TEST: batch:60,epoch:36,loss:1.2923846244812012\n",
      "TEST: batch:70,epoch:36,loss:1.2055537700653076\n",
      "TEST: batch:80,epoch:36,loss:1.3853017091751099\n",
      "TRAIN: batch:50,epoch:37,loss:1.0257474184036255\n",
      "TRAIN: batch:100,epoch:37,loss:1.0573369264602661\n",
      "TRAIN: batch:150,epoch:37,loss:1.0532631874084473\n",
      "TRAIN: batch:200,epoch:37,loss:1.1079155206680298\n",
      "TRAIN: batch:250,epoch:37,loss:1.0762372016906738\n",
      "TRAIN: batch:300,epoch:37,loss:1.073432207107544\n",
      "TRAIN: batch:350,epoch:37,loss:1.0345714092254639\n",
      "TRAIN: batch:400,epoch:37,loss:1.098588228225708\n",
      "TRAIN: batch:450,epoch:37,loss:1.1144299507141113\n",
      "TRAIN: batch:500,epoch:37,loss:1.0965416431427002\n",
      "TRAIN: batch:550,epoch:37,loss:1.0059565305709839\n",
      "TRAIN: batch:600,epoch:37,loss:1.0984010696411133\n",
      "TRAIN: batch:650,epoch:37,loss:1.0860986709594727\n",
      "TRAIN: batch:700,epoch:37,loss:1.0793344974517822\n",
      "TRAIN: batch:750,epoch:37,loss:1.028632402420044\n",
      "TEST: batch:10,epoch:37,loss:1.327690839767456\n",
      "TEST: batch:20,epoch:37,loss:1.3489048480987549\n",
      "TEST: batch:30,epoch:37,loss:1.292822241783142\n",
      "TEST: batch:40,epoch:37,loss:1.320155143737793\n",
      "TEST: batch:50,epoch:37,loss:1.3163756132125854\n",
      "TEST: batch:60,epoch:37,loss:1.2867435216903687\n",
      "TEST: batch:70,epoch:37,loss:1.2018864154815674\n",
      "TEST: batch:80,epoch:37,loss:1.3934952020645142\n",
      "TRAIN: batch:50,epoch:38,loss:1.025571346282959\n",
      "TRAIN: batch:100,epoch:38,loss:1.0635132789611816\n",
      "TRAIN: batch:150,epoch:38,loss:1.0536484718322754\n",
      "TRAIN: batch:200,epoch:38,loss:1.1145275831222534\n",
      "TRAIN: batch:250,epoch:38,loss:1.0594873428344727\n",
      "TRAIN: batch:300,epoch:38,loss:1.0749000310897827\n",
      "TRAIN: batch:350,epoch:38,loss:1.0394920110702515\n",
      "TRAIN: batch:400,epoch:38,loss:1.0958163738250732\n",
      "TRAIN: batch:450,epoch:38,loss:1.119754433631897\n",
      "TRAIN: batch:500,epoch:38,loss:1.079217553138733\n",
      "TRAIN: batch:550,epoch:38,loss:1.0062745809555054\n",
      "TRAIN: batch:600,epoch:38,loss:1.097521424293518\n",
      "TRAIN: batch:650,epoch:38,loss:1.0926198959350586\n",
      "TRAIN: batch:700,epoch:38,loss:1.0656044483184814\n",
      "TRAIN: batch:750,epoch:38,loss:1.0332599878311157\n",
      "TEST: batch:10,epoch:38,loss:1.3295624256134033\n",
      "TEST: batch:20,epoch:38,loss:1.3599783182144165\n",
      "TEST: batch:30,epoch:38,loss:1.2840244770050049\n",
      "TEST: batch:40,epoch:38,loss:1.3265483379364014\n",
      "TEST: batch:50,epoch:38,loss:1.3107601404190063\n",
      "TEST: batch:60,epoch:38,loss:1.2860724925994873\n",
      "TEST: batch:70,epoch:38,loss:1.2026311159133911\n",
      "TEST: batch:80,epoch:38,loss:1.3675334453582764\n",
      "TRAIN: batch:50,epoch:39,loss:1.0307799577713013\n",
      "TRAIN: batch:100,epoch:39,loss:1.0549466609954834\n",
      "TRAIN: batch:150,epoch:39,loss:1.051826000213623\n",
      "TRAIN: batch:200,epoch:39,loss:1.1030476093292236\n",
      "TRAIN: batch:250,epoch:39,loss:1.059730887413025\n",
      "TRAIN: batch:300,epoch:39,loss:1.0783205032348633\n",
      "TRAIN: batch:350,epoch:39,loss:1.0352364778518677\n",
      "TRAIN: batch:400,epoch:39,loss:1.0878828763961792\n",
      "TRAIN: batch:450,epoch:39,loss:1.1129603385925293\n",
      "TRAIN: batch:500,epoch:39,loss:1.0858793258666992\n",
      "TRAIN: batch:550,epoch:39,loss:0.999049961566925\n",
      "TRAIN: batch:600,epoch:39,loss:1.094801425933838\n",
      "TRAIN: batch:650,epoch:39,loss:1.084263801574707\n",
      "TRAIN: batch:700,epoch:39,loss:1.0800520181655884\n",
      "TRAIN: batch:750,epoch:39,loss:1.036686658859253\n",
      "TEST: batch:10,epoch:39,loss:1.330722689628601\n",
      "TEST: batch:20,epoch:39,loss:1.3582676649093628\n",
      "TEST: batch:30,epoch:39,loss:1.2936820983886719\n",
      "TEST: batch:40,epoch:39,loss:1.3140586614608765\n",
      "TEST: batch:50,epoch:39,loss:1.3173720836639404\n",
      "TEST: batch:60,epoch:39,loss:1.2840733528137207\n",
      "TEST: batch:70,epoch:39,loss:1.2015076875686646\n",
      "TEST: batch:80,epoch:39,loss:1.3769917488098145\n",
      "TRAIN: batch:50,epoch:40,loss:1.0066906213760376\n",
      "TRAIN: batch:100,epoch:40,loss:1.0468721389770508\n",
      "TRAIN: batch:150,epoch:40,loss:1.0491316318511963\n",
      "TRAIN: batch:200,epoch:40,loss:1.0979831218719482\n",
      "TRAIN: batch:250,epoch:40,loss:1.066865086555481\n",
      "TRAIN: batch:300,epoch:40,loss:1.0736147165298462\n",
      "TRAIN: batch:350,epoch:40,loss:1.0337127447128296\n",
      "TRAIN: batch:400,epoch:40,loss:1.0929780006408691\n",
      "TRAIN: batch:450,epoch:40,loss:1.1064159870147705\n",
      "TRAIN: batch:500,epoch:40,loss:1.0818209648132324\n",
      "TRAIN: batch:550,epoch:40,loss:0.9934567809104919\n",
      "TRAIN: batch:600,epoch:40,loss:1.0906493663787842\n",
      "TRAIN: batch:650,epoch:40,loss:1.0755192041397095\n",
      "TRAIN: batch:700,epoch:40,loss:1.0641690492630005\n",
      "TRAIN: batch:750,epoch:40,loss:1.0276585817337036\n",
      "TEST: batch:10,epoch:40,loss:1.3300198316574097\n",
      "TEST: batch:20,epoch:40,loss:1.3591152429580688\n",
      "TEST: batch:30,epoch:40,loss:1.2829411029815674\n",
      "TEST: batch:40,epoch:40,loss:1.3170864582061768\n",
      "TEST: batch:50,epoch:40,loss:1.315674901008606\n",
      "TEST: batch:60,epoch:40,loss:1.2974400520324707\n",
      "TEST: batch:70,epoch:40,loss:1.2079954147338867\n",
      "TEST: batch:80,epoch:40,loss:1.3769055604934692\n",
      "TRAIN: batch:50,epoch:41,loss:1.025468349456787\n",
      "TRAIN: batch:100,epoch:41,loss:1.046648383140564\n",
      "TRAIN: batch:150,epoch:41,loss:1.0512408018112183\n",
      "TRAIN: batch:200,epoch:41,loss:1.101547360420227\n",
      "TRAIN: batch:250,epoch:41,loss:1.0454306602478027\n",
      "TRAIN: batch:300,epoch:41,loss:1.0805693864822388\n",
      "TRAIN: batch:350,epoch:41,loss:1.0346996784210205\n",
      "TRAIN: batch:400,epoch:41,loss:1.0833734273910522\n",
      "TRAIN: batch:450,epoch:41,loss:1.103633165359497\n",
      "TRAIN: batch:500,epoch:41,loss:1.07362699508667\n",
      "TRAIN: batch:550,epoch:41,loss:0.9924009442329407\n",
      "TRAIN: batch:600,epoch:41,loss:1.0953583717346191\n",
      "TRAIN: batch:650,epoch:41,loss:1.0818876028060913\n",
      "TRAIN: batch:700,epoch:41,loss:1.0616343021392822\n",
      "TRAIN: batch:750,epoch:41,loss:1.026521921157837\n",
      "TEST: batch:10,epoch:41,loss:1.3176560401916504\n",
      "TEST: batch:20,epoch:41,loss:1.3477368354797363\n",
      "TEST: batch:30,epoch:41,loss:1.2831212282180786\n",
      "TEST: batch:40,epoch:41,loss:1.334987998008728\n",
      "TEST: batch:50,epoch:41,loss:1.3176467418670654\n",
      "TEST: batch:60,epoch:41,loss:1.2827702760696411\n",
      "TEST: batch:70,epoch:41,loss:1.2171274423599243\n",
      "TEST: batch:80,epoch:41,loss:1.388948678970337\n",
      "TRAIN: batch:50,epoch:42,loss:1.0169918537139893\n",
      "TRAIN: batch:100,epoch:42,loss:1.0425307750701904\n",
      "TRAIN: batch:150,epoch:42,loss:1.0403780937194824\n",
      "TRAIN: batch:200,epoch:42,loss:1.0883285999298096\n",
      "TRAIN: batch:250,epoch:42,loss:1.0520342588424683\n",
      "TRAIN: batch:300,epoch:42,loss:1.0671004056930542\n",
      "TRAIN: batch:350,epoch:42,loss:1.0316187143325806\n",
      "TRAIN: batch:400,epoch:42,loss:1.094456434249878\n",
      "TRAIN: batch:450,epoch:42,loss:1.1092982292175293\n",
      "TRAIN: batch:500,epoch:42,loss:1.0748045444488525\n",
      "TRAIN: batch:550,epoch:42,loss:0.9846295118331909\n",
      "TRAIN: batch:600,epoch:42,loss:1.0946934223175049\n",
      "TRAIN: batch:650,epoch:42,loss:1.0759799480438232\n",
      "TRAIN: batch:700,epoch:42,loss:1.0601801872253418\n",
      "TRAIN: batch:750,epoch:42,loss:1.0178284645080566\n",
      "TEST: batch:10,epoch:42,loss:1.3287062644958496\n",
      "TEST: batch:20,epoch:42,loss:1.348708987236023\n",
      "TEST: batch:30,epoch:42,loss:1.3014743328094482\n",
      "TEST: batch:40,epoch:42,loss:1.3142045736312866\n",
      "TEST: batch:50,epoch:42,loss:1.295346975326538\n",
      "TEST: batch:60,epoch:42,loss:1.2805290222167969\n",
      "TEST: batch:70,epoch:42,loss:1.216410756111145\n",
      "TEST: batch:80,epoch:42,loss:1.3521559238433838\n",
      "TRAIN: batch:50,epoch:43,loss:1.0133047103881836\n",
      "TRAIN: batch:100,epoch:43,loss:1.0485742092132568\n",
      "TRAIN: batch:150,epoch:43,loss:1.0423283576965332\n",
      "TRAIN: batch:200,epoch:43,loss:1.0913788080215454\n",
      "TRAIN: batch:250,epoch:43,loss:1.0483355522155762\n",
      "TRAIN: batch:300,epoch:43,loss:1.073651909828186\n",
      "TRAIN: batch:350,epoch:43,loss:1.0337328910827637\n",
      "TRAIN: batch:400,epoch:43,loss:1.0826393365859985\n",
      "TRAIN: batch:450,epoch:43,loss:1.1014665365219116\n",
      "TRAIN: batch:500,epoch:43,loss:1.0689142942428589\n",
      "TRAIN: batch:550,epoch:43,loss:0.9931038618087769\n",
      "TRAIN: batch:600,epoch:43,loss:1.0836089849472046\n",
      "TRAIN: batch:650,epoch:43,loss:1.0800095796585083\n",
      "TRAIN: batch:700,epoch:43,loss:1.0618822574615479\n",
      "TRAIN: batch:750,epoch:43,loss:1.0171340703964233\n",
      "TEST: batch:10,epoch:43,loss:1.3330633640289307\n",
      "TEST: batch:20,epoch:43,loss:1.3553433418273926\n",
      "TEST: batch:30,epoch:43,loss:1.280391812324524\n",
      "TEST: batch:40,epoch:43,loss:1.3086662292480469\n",
      "TEST: batch:50,epoch:43,loss:1.3025354146957397\n",
      "TEST: batch:60,epoch:43,loss:1.2892999649047852\n",
      "TEST: batch:70,epoch:43,loss:1.2076011896133423\n",
      "TEST: batch:80,epoch:43,loss:1.3784034252166748\n",
      "TRAIN: batch:50,epoch:44,loss:1.013925552368164\n",
      "TRAIN: batch:100,epoch:44,loss:1.0423710346221924\n",
      "TRAIN: batch:150,epoch:44,loss:1.0360404253005981\n",
      "TRAIN: batch:200,epoch:44,loss:1.0959444046020508\n",
      "TRAIN: batch:250,epoch:44,loss:1.0538716316223145\n",
      "TRAIN: batch:300,epoch:44,loss:1.065946340560913\n",
      "TRAIN: batch:350,epoch:44,loss:1.0314308404922485\n",
      "TRAIN: batch:400,epoch:44,loss:1.0849751234054565\n",
      "TRAIN: batch:450,epoch:44,loss:1.1019792556762695\n",
      "TRAIN: batch:500,epoch:44,loss:1.0683666467666626\n",
      "TRAIN: batch:550,epoch:44,loss:0.9930101633071899\n",
      "TRAIN: batch:600,epoch:44,loss:1.0873382091522217\n",
      "TRAIN: batch:650,epoch:44,loss:1.0723437070846558\n",
      "TRAIN: batch:700,epoch:44,loss:1.049107313156128\n",
      "TRAIN: batch:750,epoch:44,loss:1.014077067375183\n",
      "TEST: batch:10,epoch:44,loss:1.3279920816421509\n",
      "TEST: batch:20,epoch:44,loss:1.3471097946166992\n",
      "TEST: batch:30,epoch:44,loss:1.2821564674377441\n",
      "TEST: batch:40,epoch:44,loss:1.3138738870620728\n",
      "TEST: batch:50,epoch:44,loss:1.3132396936416626\n",
      "TEST: batch:60,epoch:44,loss:1.2893028259277344\n",
      "TEST: batch:70,epoch:44,loss:1.2080599069595337\n",
      "TEST: batch:80,epoch:44,loss:1.3856773376464844\n",
      "TRAIN: batch:50,epoch:45,loss:1.016726016998291\n",
      "TRAIN: batch:100,epoch:45,loss:1.0382509231567383\n",
      "TRAIN: batch:150,epoch:45,loss:1.044243574142456\n",
      "TRAIN: batch:200,epoch:45,loss:1.0826640129089355\n",
      "TRAIN: batch:250,epoch:45,loss:1.0446549654006958\n",
      "TRAIN: batch:300,epoch:45,loss:1.0645066499710083\n",
      "TRAIN: batch:350,epoch:45,loss:1.0245777368545532\n",
      "TRAIN: batch:400,epoch:45,loss:1.0765774250030518\n",
      "TRAIN: batch:450,epoch:45,loss:1.0978479385375977\n",
      "TRAIN: batch:500,epoch:45,loss:1.0677406787872314\n",
      "TRAIN: batch:550,epoch:45,loss:0.9841676950454712\n",
      "TRAIN: batch:600,epoch:45,loss:1.0869288444519043\n",
      "TRAIN: batch:650,epoch:45,loss:1.0750724077224731\n",
      "TRAIN: batch:700,epoch:45,loss:1.0552769899368286\n",
      "TRAIN: batch:750,epoch:45,loss:1.0148766040802002\n",
      "TEST: batch:10,epoch:45,loss:1.3188749551773071\n",
      "TEST: batch:20,epoch:45,loss:1.3492110967636108\n",
      "TEST: batch:30,epoch:45,loss:1.2759195566177368\n",
      "TEST: batch:40,epoch:45,loss:1.3074617385864258\n",
      "TEST: batch:50,epoch:45,loss:1.30915105342865\n",
      "TEST: batch:60,epoch:45,loss:1.2852483987808228\n",
      "TEST: batch:70,epoch:45,loss:1.203133463859558\n",
      "TEST: batch:80,epoch:45,loss:1.3764946460723877\n",
      "TRAIN: batch:50,epoch:46,loss:1.0068862438201904\n",
      "TRAIN: batch:100,epoch:46,loss:1.0429273843765259\n",
      "TRAIN: batch:150,epoch:46,loss:1.0372673273086548\n",
      "TRAIN: batch:200,epoch:46,loss:1.0841556787490845\n",
      "TRAIN: batch:250,epoch:46,loss:1.0373473167419434\n",
      "TRAIN: batch:300,epoch:46,loss:1.0705015659332275\n",
      "TRAIN: batch:350,epoch:46,loss:1.020034670829773\n",
      "TRAIN: batch:400,epoch:46,loss:1.0770390033721924\n",
      "TRAIN: batch:450,epoch:46,loss:1.0989824533462524\n",
      "TRAIN: batch:500,epoch:46,loss:1.0733528137207031\n",
      "TRAIN: batch:550,epoch:46,loss:0.9897525906562805\n",
      "TRAIN: batch:600,epoch:46,loss:1.0804871320724487\n",
      "TRAIN: batch:650,epoch:46,loss:1.06765878200531\n",
      "TRAIN: batch:700,epoch:46,loss:1.0566264390945435\n",
      "TRAIN: batch:750,epoch:46,loss:1.0135791301727295\n",
      "TEST: batch:10,epoch:46,loss:1.333121657371521\n",
      "TEST: batch:20,epoch:46,loss:1.3787494897842407\n",
      "TEST: batch:30,epoch:46,loss:1.2971012592315674\n",
      "TEST: batch:40,epoch:46,loss:1.3229284286499023\n",
      "TEST: batch:50,epoch:46,loss:1.3155490159988403\n",
      "TEST: batch:60,epoch:46,loss:1.2951871156692505\n",
      "TEST: batch:70,epoch:46,loss:1.2165703773498535\n",
      "TEST: batch:80,epoch:46,loss:1.3680648803710938\n",
      "TRAIN: batch:50,epoch:47,loss:1.0153285264968872\n",
      "TRAIN: batch:100,epoch:47,loss:1.0466667413711548\n",
      "TRAIN: batch:150,epoch:47,loss:1.0301092863082886\n",
      "TRAIN: batch:200,epoch:47,loss:1.0873655080795288\n",
      "TRAIN: batch:250,epoch:47,loss:1.0453128814697266\n",
      "TRAIN: batch:300,epoch:47,loss:1.0603206157684326\n",
      "TRAIN: batch:350,epoch:47,loss:1.0173139572143555\n",
      "TRAIN: batch:400,epoch:47,loss:1.0795809030532837\n",
      "TRAIN: batch:450,epoch:47,loss:1.102159023284912\n",
      "TRAIN: batch:500,epoch:47,loss:1.0705180168151855\n",
      "TRAIN: batch:550,epoch:47,loss:0.9887523055076599\n",
      "TRAIN: batch:600,epoch:47,loss:1.0927467346191406\n",
      "TRAIN: batch:650,epoch:47,loss:1.0651237964630127\n",
      "TRAIN: batch:700,epoch:47,loss:1.0582255125045776\n",
      "TRAIN: batch:750,epoch:47,loss:1.0101820230484009\n",
      "TEST: batch:10,epoch:47,loss:1.3262269496917725\n",
      "TEST: batch:20,epoch:47,loss:1.3627465963363647\n",
      "TEST: batch:30,epoch:47,loss:1.2957487106323242\n",
      "TEST: batch:40,epoch:47,loss:1.3377394676208496\n",
      "TEST: batch:50,epoch:47,loss:1.3193328380584717\n",
      "TEST: batch:60,epoch:47,loss:1.3032492399215698\n",
      "TEST: batch:70,epoch:47,loss:1.223943829536438\n",
      "TEST: batch:80,epoch:47,loss:1.3790600299835205\n",
      "TRAIN: batch:50,epoch:48,loss:1.0123406648635864\n",
      "TRAIN: batch:100,epoch:48,loss:1.0375721454620361\n",
      "TRAIN: batch:150,epoch:48,loss:1.0347778797149658\n",
      "TRAIN: batch:200,epoch:48,loss:1.0800533294677734\n",
      "TRAIN: batch:250,epoch:48,loss:1.04433274269104\n",
      "TRAIN: batch:300,epoch:48,loss:1.0644617080688477\n",
      "TRAIN: batch:350,epoch:48,loss:1.0279603004455566\n",
      "TRAIN: batch:400,epoch:48,loss:1.0750741958618164\n",
      "TRAIN: batch:450,epoch:48,loss:1.1010518074035645\n",
      "TRAIN: batch:500,epoch:48,loss:1.0599408149719238\n",
      "TRAIN: batch:550,epoch:48,loss:0.9771887063980103\n",
      "TRAIN: batch:600,epoch:48,loss:1.082663893699646\n",
      "TRAIN: batch:650,epoch:48,loss:1.068490743637085\n",
      "TRAIN: batch:700,epoch:48,loss:1.0550227165222168\n",
      "TRAIN: batch:750,epoch:48,loss:1.0171221494674683\n",
      "TEST: batch:10,epoch:48,loss:1.3383779525756836\n",
      "TEST: batch:20,epoch:48,loss:1.353703498840332\n",
      "TEST: batch:30,epoch:48,loss:1.2811814546585083\n",
      "TEST: batch:40,epoch:48,loss:1.3224132061004639\n",
      "TEST: batch:50,epoch:48,loss:1.3112772703170776\n",
      "TEST: batch:60,epoch:48,loss:1.2899595499038696\n",
      "TEST: batch:70,epoch:48,loss:1.204994797706604\n",
      "TEST: batch:80,epoch:48,loss:1.3787870407104492\n",
      "TRAIN: batch:50,epoch:49,loss:1.0028986930847168\n",
      "TRAIN: batch:100,epoch:49,loss:1.023543357849121\n",
      "TRAIN: batch:150,epoch:49,loss:1.0319472551345825\n",
      "TRAIN: batch:200,epoch:49,loss:1.0799585580825806\n",
      "TRAIN: batch:250,epoch:49,loss:1.0419516563415527\n",
      "TRAIN: batch:300,epoch:49,loss:1.0649745464324951\n",
      "TRAIN: batch:350,epoch:49,loss:1.012995958328247\n",
      "TRAIN: batch:400,epoch:49,loss:1.071624755859375\n",
      "TRAIN: batch:450,epoch:49,loss:1.0983175039291382\n",
      "TRAIN: batch:500,epoch:49,loss:1.0556950569152832\n",
      "TRAIN: batch:550,epoch:49,loss:0.9799080491065979\n",
      "TRAIN: batch:600,epoch:49,loss:1.093478798866272\n",
      "TRAIN: batch:650,epoch:49,loss:1.0573424100875854\n",
      "TRAIN: batch:700,epoch:49,loss:1.051961898803711\n",
      "TRAIN: batch:750,epoch:49,loss:1.0136016607284546\n",
      "TEST: batch:10,epoch:49,loss:1.3410214185714722\n",
      "TEST: batch:20,epoch:49,loss:1.3405646085739136\n",
      "TEST: batch:30,epoch:49,loss:1.2850779294967651\n",
      "TEST: batch:40,epoch:49,loss:1.3022329807281494\n",
      "TEST: batch:50,epoch:49,loss:1.3108552694320679\n",
      "TEST: batch:60,epoch:49,loss:1.283532738685608\n",
      "TEST: batch:70,epoch:49,loss:1.2071341276168823\n",
      "TEST: batch:80,epoch:49,loss:1.3942936658859253\n"
     ]
    }
   ],
   "source": [
    "epoch_train=[]\n",
    "epoch_test=[]\n",
    "epoch_time=[]\n",
    "epoch_train_time=[]\n",
    "epoch_test_time=[]\n",
    "print(f'Are we on cuda? {torch.cuda.is_available()}')\n",
    "start_time=time.time()\n",
    "for e in range(epochs):\n",
    "    train_batches=create_batches(train_data,batch_size,seq_len)\n",
    "    test_batches=create_batches(test_data,batch_size,seq_len)\n",
    "    train_acc=0\n",
    "    test_acc=0\n",
    "    hidden_node=lstm_model.create_hidden(batch_size)\n",
    "    \n",
    "    for ind,(x_train,y_train) in enumerate(train_batches):\n",
    "        ind+=1\n",
    "        #One-hot encoding of X\n",
    "        #Convert y to torch\n",
    "#         print('x train shape',x_train.shape)\n",
    "        x_train=one_hot_encoding(x_train,max(encode_text)+1)\n",
    "#         print('x train shape',x_train.shape)\n",
    "#         print('hidden train shape',hidden_node[0].shape)\n",
    "        y_train=torch.from_numpy(y_train)\n",
    "        if torch.cuda.is_available():\n",
    "            x_train=x_train.cuda()\n",
    "            y_train=y_train.cuda()\n",
    "        hidden_node = tuple([state.data for state in hidden_node])\n",
    "        lstm_model.zero_grad()\n",
    "        y_train_pred,hidden_node=lstm_model.forward(x_train,hidden_node)\n",
    "        \n",
    "        loss=criterion(y_train_pred,y_train.view(batch_size*seq_len).long())\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(lstm_model.parameters(),max_norm=5)\n",
    "        optimizer.step()\n",
    "#         hidden_node = tuple([i for i in template_node])\n",
    "        if ind%50==0:\n",
    "            print(f'TRAIN: batch:{ind},epoch:{e},loss:{loss}')\n",
    "    epoch_train.append(loss)\n",
    "    epoch_train_time.append(time.time())\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for ind,(x_test,y_test) in enumerate(test_batches):\n",
    "            ind+=1\n",
    "            x_test=one_hot_encoding(x_test,max(encode_text)+1)\n",
    "            y_test=torch.from_numpy(y_test)\n",
    "            if torch.cuda.is_available():\n",
    "                x_test=x_test.cuda()\n",
    "                y_test=y_test.cuda()\n",
    "            y_test_pred,hidden_node=lstm_model(x_test,hidden_node)\n",
    "\n",
    "            loss_test=criterion(y_test_pred,y_test.view(batch_size*seq_len).long())\n",
    "#             nn.utils.clip_grad_norm_(lstm_model.parameters(),max_norm=5)\n",
    "            if ind%10==0:\n",
    "                print(f'TEST: batch:{ind},epoch:{e},loss:{loss_test}')\n",
    "        epoch_test.append(loss_test)\n",
    "        epoch_test_time.append(time.time())\n",
    "        epoch_time.append(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='lstm_char_hidden512_layers3.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3674.4282)\n",
      "tensor(-996.0383)\n",
      "tensor(-228.1136)\n",
      "tensor(-227.3977)\n",
      "tensor(-1449.0662)\n",
      "tensor(-32.7306)\n",
      "tensor(-349.5423)\n",
      "tensor(-346.7842)\n",
      "tensor(936.5356)\n",
      "tensor(271.3689)\n",
      "tensor(33.5938)\n",
      "tensor(32.7236)\n",
      "tensor(393.1368)\n",
      "tensor(-937.8214)\n",
      "tensor(38.2691)\n",
      "tensor(38.1144)\n",
      "tensor(-437.1507)\n",
      "tensor(-496.6007)\n",
      "tensor(-12.3982)\n",
      "tensor(-12.0088)\n",
      "tensor(2.5857)\n",
      "tensor(-15.5364)\n"
     ]
    }
   ],
   "source": [
    "for key in lstm_model.state_dict().keys():\n",
    "    print(lstm_model.state_dict()[key].cpu().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(),name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers={}\n",
    "hypers={\n",
    "    'hidden:':512,\n",
    "        'layers':3,\n",
    "        'train_losses':[e.item() for e in epoch_train],\n",
    "        'test_losses':[e.item() for e in epoch_test],\n",
    "        'epoch_time':epoch_time,\n",
    "        'epoch_train_time':epoch_train_time, \n",
    "        'epoch_test_time':epoch_test_time\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lstm_hyperparams.json', 'w+') as f:\n",
    "    # this would place the entire output on one line\n",
    "    # use json.dump(lista_items, f, indent=4) to \"pretty-print\" with four spaces per indent\n",
    "    json.dump(hypers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs),epoch_train,'r')\n",
    "plt.plot(range(epochs),epoch_test,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
